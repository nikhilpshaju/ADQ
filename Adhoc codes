Payable to project companies
Retention payables
Accounts payable to related parties and third parties
Deferred revenue/income
Insurance contract liabilities
Contract liabilities


You are a financial-statement commentary evaluator.

Your task:
Check whether the commentary accurately explains the variance shown in a financial statement pack.

Inputs provided:
- Sheet type: SOFP | SOPL | SOCIE | GN18.1.1 | GN17.3
- Variance value (positive = increase, negative = decrease)
- Commentary text

--------------------------------------------------
GENERAL EVALUATION RULES
--------------------------------------------------
1. Determine whether the variance represents an increase or a decrease.
2. Check whether the commentary:
   a. States or implies the correct direction of movement.
   b. Provides a logical explanation for the variance based on the sheet type.
   c. Does not contradict the numerical direction of the variance.
   d. Contains no irrelevant, filler, or placeholder text (e.g., “xxxx”).
3. Give a verdict: Correct or Incorrect.
4. Provide a brief justification.

--------------------------------------------------
SHEET-SPECIFIC LOGIC
--------------------------------------------------

SOFP (Statement of Financial Position) logic:
- Assets increase due to purchases, revaluations, or accruals.
- Assets decrease due to depreciation, disposals, or impairments.
- Liabilities increase due to new obligations.
- Liabilities decrease due to repayments or settlements.
- Provisions:
  - Payments reduce provisions.
  - New estimates increase provisions.
  - Release of excess provisions decreases them.

SOPL (Statement of Profit or Loss) logic:
- Revenue increases explained by volume, price, timing, mix, or FX.
- Revenue decreases explained by lower volume, lower price, loss of customers, etc.
- Costs increase due to inflation, activity, inefficiencies, or one-offs.
- Costs decrease due to savings, efficiencies, or reduced activity.
- Profit variance must follow revenue and cost changes logically.

SOCIE (Statement of Changes in Equity) logic:
- Variance should relate to:
  - Profit for the year
  - Dividends declared/paid
  - Share issues or buybacks
  - Revaluation gains/losses
  - FX or OCI movements
- Commentary must correctly link variance to the appropriate equity reserve.

GN18.1.1 (Property, Plant, and Equipment Costs):
- Variances may result from:
  - Additions/purchases of PPE
  - Disposals or sales of assets
  - Depreciation charge movements
  - Revaluations or impairments
- Commentary must correctly reference these drivers and match the direction of variance.

GN17.3 (Loans, Borrowings, and Lease Liabilities):
- Variances may result from:
  - New borrowings or lease agreements
  - Repayments of existing loans or leases
  - Interest accruals or adjustments
- Commentary must correctly reference these drivers and match the direction of variance.

--------------------------------------------------
OUTPUT FORMAT
--------------------------------------------------
Return your assessment in this JSON structure:

{
  "verdict": "Correct" | "Incorrect",
  "reason": "Brief explanation of why the commentary matches or fails the numerical and accounting logic.",
  "variance_direction": "Increase" | "Decrease"
}






print(file_path)

wb = load_workbook(file_path)

# Sheets to process
sheets_to_process = [s for s in wb.sheetnames if s.lower().startswith("output - rule") or s.lower().startswith("output - sdm & tm")]
print(sheets_to_process)

# Styles
header_fill = PatternFill(start_color="151B54", end_color="151B54", fill_type="solid")
header_font = Font(bold=True, color="FFFFFF")
pattern_fill = PatternFill(start_color="ECECEC", end_color="ECECEC", fill_type="solid")
pattern_font = Font(bold=True, color="000000")
green_fill = PatternFill(start_color="93DC5C", end_color="93DC5C", fill_type="solid")
red_fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
soft_delete_fill = PatternFill(start_color="FFFFFF", end_color="FFFFFF", fill_type="solid")
soft_delete_font = Font(color="FFFFFF", italic=True)
thin_border = Border(
    left=Side(style="thin"),
    right=Side(style="thin"),
    top=Side(style="thin"),
    bottom=Side(style="thin")
)
def apply_sdm_tm_format(ws):
    print(f"Applying SDM/TM block: {ws.title}")
    max_row = ws.max_row
    max_col = ws.max_column

    phrase = [
        "all forms in SDM report have been completed for the entity",
        "all tasks in task manager report have been completed for the entity",
        "sdm file not found in sharepoint",
        "task manager file not found in sharepoint"
    ]
    row=1
    while row <= max_row:
        
        cell_b = ws.cell(row=row, column=2)  # column B
        val = cell_b.value
        if val and isinstance(val, str):
            clean=val.strip().lower()

            if any(p in clean for p in phrase):
                #Applying bold only
                cell_b.font = Font(bold=True)
                row += 1
                print("made it bold only")
                continue

        if val and isinstance(val, str) and val.strip().lower().startswith("below"):             
                cell_b.font = Font(bold=True)
                header_row_idx = row + 1
                while header_row_idx <= max_row and (ws.cell(row=header_row_idx, column=1).value is None and ws.cell(row=header_row_idx, column=2).value is None):
                    header_row_idx += 1

                if header_row_idx <= max_row:
                    for c in range(1, max_col + 1):
                        hcell = ws.cell(row=header_row_idx, column=c)
                        if hcell.value is not None and str(hcell.value).strip() != "":
                            hcell.fill = header_fill
                            hcell.font = header_font
                            hcell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

                    # applying table borders to rows after header 
                    data_row = header_row_idx + 1
                    while data_row <= max_row:
                        b_val = ws.cell(row=data_row, column=2).value
                        row_blank = all((ws.cell(row=data_row, column=col).value is None or str(ws.cell(row=data_row, column=col).value).strip() == "") for col in range(1, max_col + 1))
                        if (b_val and isinstance(b_val, str) and b_val.strip().lower().startswith("below")) or row_blank:
                            break

                        for c in range(2, max_col + 1):
                            dcell = ws.cell(row=data_row, column=c)
                            dcell.border = Border(left=Side(style="thin"), right=Side(style="thin"),
                                                top=Side(style="thin"), bottom=Side(style="thin"))
                            dcell.alignment = Alignment(wrap_text=True, vertical="top")
                        data_row += 1
                    row = data_row
                    continue            
        row += 1

    print(f"Completed SDM/TM formatting on {ws.title}")

sr_regex = re.compile(r"\bsr\.?\s*no\.?\b", re.I)


def normalize_header_text(s):
    return re.sub(r"[^A-Za-z0-9]", "", str(s or "")).lower()

def soft_delete_cell(cell):
    cell.fill = soft_delete_fill
    cell.font = soft_delete_font
    cell.value = ""

for sheet_name in sheets_to_process:
    ws = wb[sheet_name]
    print(f"Processing sheet: {ws.title}")

    ws.sheet_view.showGridLines = False
    ws.sheet_view.zoomScale = 70
    ws.sheet_properties.tabColor = "081F59"


    # Finding last used row and column
    last_row = 0
    last_col = 0
    for row in ws.iter_rows():
        for cell in row:
            if cell.value not in (None, ""):
                last_row = max(last_row, cell.row)
                last_col = max(last_col, cell.column)

    # Hide unused columns
    for col in range(last_col + 1, ws.max_column + 1):
        ws.column_dimensions[get_column_letter(col)].hidden = True
    # Hide unused rows
    for r in range(last_row + 1, ws.max_row + 1):
        ws.row_dimensions[r].hidden = True

    try:
        for r in [2, 3, 4]:
            if r <= ws.max_row:
                for cell in ws[r]:
                    cell.font = Font(bold=True, size=18)
        if 6 <= ws.max_row:
            for cell in ws[6]:
                cell.font = Font(bold=True)
        if 8 <= ws.max_row:
            for cell in ws[8]:
                cell.font = Font(bold=True)       
    except Exception as e:
        print(f"Row-formatting skipped for {sheet_name}: {e}")

    if "output - sdm" in sheet_name.lower():
        apply_sdm_tm_format(ws)

    # Row scanning
    row = 1
    while row <= last_row:
        raw_texts = [str(c.value).strip() if c.value else "" for c in ws[row]]
        header_norms = [normalize_header_text(txt) for txt in raw_texts]

        # Detect SR No row
        sr_col_idx = next((idx for idx, txt in enumerate(raw_texts) if txt and sr_regex.search(txt)), None)
        if sr_col_idx is not None:
            pattern_row_idx = row - 1

            # Identify helper columns
            helper_cols = {name: header_norms.index(name) for name in ["rownumber", "cellnumber", "sheetname"] if name in header_norms}

            # Pattern row formatting
            first_helper_col = min(helper_cols.values()) if helper_cols else len(header_norms)
            for idx, cell in enumerate(ws[pattern_row_idx][1:], start=1):
                if idx >= first_helper_col:
                    break
                cell.fill = pattern_fill
                cell.font = pattern_font
                left = Side(style="medium") if idx == 1 else Side(style=None)
                right = Side(style="medium") if idx == first_helper_col - 1 else Side(style=None)
                cell.border = Border(top=Side(style="medium"), bottom=Side(style="medium"), left=left, right=right)

            # Header row formatting
            for idx, cell in enumerate(ws[row][1:], start=1):
                if idx in helper_cols.values():
                    continue
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal="center", vertical="center")
                cell.border = thin_border

            # Soft-delete helper header cells
            for helper_col in helper_cols.values():
                if helper_col < len(ws[row]):
                    soft_delete_cell(ws[row][helper_col])

            # Hyperlink-related columns
            link_col_index = helper_cols.get("rownumber") or helper_cols.get("cellnumber")
            header_type = "rownumber" if "rownumber" in helper_cols else "cellnumber" if "cellnumber" in helper_cols else None
            sheetname_col_index = helper_cols.get("sheetname")

            # Identify target columns based on sheet
            notename_col_idx = header_norms.index("notename") if "notename" in header_norms else None
            desc_col_idx = header_norms.index("notenamedescription") if "notenamedescription" in header_norms else None
            account_col_idx = header_norms.index("accountname") if "accountname" in header_norms else None
            remarks_col_idx = header_norms.index("remarks") if "remarks" in header_norms else None

            # TB Value and Amount column indexes
            tb_value_idx = header_norms.index("tbvalue") if "tbvalue" in header_norms else None
            amount_idx = header_norms.index("amount") if "amount" in header_norms else None

            # Define target columns for hyperlinks based on sheet name
            if "output - rule 1" in sheet_name.lower():
                target_columns = [idx for idx in [desc_col_idx] if idx is not None]
            elif "output - rule 2" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 3" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 4" in sheet_name.lower():
                target_columns = [idx for idx in [notename_col_idx] if idx is not None]

                remarks_link_target_sheet = "Output - Rule 4.1"
            else:
                target_columns = []

            # Process data rows
            data_idx = row + 1
            while data_idx <= last_row:
                data_row = ws[data_idx]
                if all((c.value is None or str(c.value).strip() == "") for c in data_row):
                    break

                # Add borders
                for idx, c in enumerate(data_row[1:], start=1):
                    if idx not in helper_cols.values():
                        c.border = thin_border

                # Conditional formatting
                header_texts = [normalize_header_text(c.value) for c in ws[row]]
                cp_idx = header_texts.index("currentperiod") if "currentperiod" in header_texts else None
                vs_idx = header_texts.index("validationstatus") if "validationstatus" in header_texts else None

                for idx in [cp_idx, vs_idx]:
                    if idx is not None and data_row[idx].value:
                        val = str(data_row[idx].value).strip().lower()
                        if val in ["pass", "tb validated"]:
                            data_row[idx].fill = green_fill
                        elif val == "fail":
                            data_row[idx].fill = red_fill

                # --- TB VALUE / AMOUNT FORMATTING ---
                for fmt_idx in [tb_value_idx, amount_idx]:
                    if fmt_idx is not None and fmt_idx < len(data_row):
                        cell = data_row[fmt_idx]
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '#,##0_);(#,##0);'

                # --- HYPERLINK LOGIC (conditional per sheet) ---
                if link_col_index is not None and sheetname_col_index is not None:
                    link_cell_value = ws.cell(row=data_idx, column=link_col_index + 1).value
                    sheetname_value = ws.cell(row=data_idx, column=sheetname_col_index + 1).value

                    if link_cell_value and sheetname_value:
                        safe_sheet_name = str(sheetname_value).strip()
                        if any(ch in safe_sheet_name for ch in [" ", "-", "."]):
                            safe_sheet_name = f"'{safe_sheet_name}'"

                        try:
                            if header_type == "rownumber":
                                target_number = int(link_cell_value)
                                target_ref = f"{safe_sheet_name}!A{target_number}:Z{target_number}"
                            elif header_type == "cellnumber":
                                target_ref = f"{safe_sheet_name}!{str(link_cell_value).upper().strip()}"
                            else:
                                target_ref = None

                            if target_ref:
                                for idx in target_columns:
                                    link_target_cell = data_row[idx]
                                    if link_target_cell and link_target_cell.value:
                                        link_target_cell.hyperlink = f"#{target_ref}"
                                        link_target_cell.font = Font(color="0000FF", underline="single")
                                        # print(f"{sheet_name}: Linked '{link_target_cell.value}' aÃƒÂ¢Ã¢â€šÂ¬Ã‚Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ {target_ref}")
                        except ValueError:
                            pass
                # Hyperlink for Rule 4 Remarks column
                if sheet_name.strip().lower() == "output - rule 4" and remarks_col_idx is not None:
                    if 'Output - Rule 4.1' in wb.sheetnames:
                        try:
                            current_row_number = data_idx  # current row 
                            remarks_cell = data_row[remarks_col_idx]
                            if remarks_cell and remarks_cell.value:
                                target_ref = f"'{remarks_link_target_sheet}'!A{current_row_number}:Z{current_row_number}"
                                remarks_cell.hyperlink = f"#{target_ref}"
                                remarks_cell.font = Font(color="0000FF", underline="single")
                                # print(f"{sheet_name}: Linked Remarks (Row {current_row_number}) Ã¢â€ â€™ {target_ref}")
                        except Exception as e:
                            print(f"{sheet_name}: Failed to link Remarks at row {data_idx} Ã¢â€ â€™ {e}")
                
                # Soft-delete helper columns
                for helper_col in helper_cols.values():
                    if helper_col < len(data_row):
                        soft_delete_cell(data_row[helper_col])

                data_idx += 1

            row = data_idx + 1
            continue
        row += 1
    
    # Autofit columns
    for col in ws.columns:
        try:
            max_length = 0
            col_letter = get_column_letter(col[0].column)
            for cell in col:
                if cell.value is not None:
                    max_length = max(max_length, len(str(cell.value)) + 4) # for changing column width
            ws.column_dimensions[col_letter].width = max(1, max_length)
        except Exception:
            pass

    try:
        
        ws.column_dimensions["A"].width = 2
        if ws.title != "Output - SDM & TM":
            ws.column_dimensions["B"].width = 6
    except Exception:
        pass

    if sheet_name.strip().lower() == "output - rule 4.1":
        print("Applying formatting to Output - Rule 4.1")

        # Adjust column widths
        col_widths = {
            "C": 10,
            "D": 25,  
            "E": 28,
            "F": 25,
            "G": 15,
            "H": 28,  
            "I": 15,
            "J": 28,
            "K": 15,
            "L": 28,
            "M": 28,
            "N": 28,
        }
        skip_rows = [2,3,4,6,8]
        for col_letter, width in col_widths.items():
            ws.column_dimensions[col_letter].width = width
        # Text wrapping 
        for row in ws.iter_rows():
            for cell in row:
                row_num = row[0].row        
                if row_num not in skip_rows:
                    cell.alignment = Alignment(wrap_text=True, vertical="top")

# Custom deletions
custom_deletions = {
    "Output - Rule 1": ["G","H"],
    "Output - Rule 2": ["I", "J"],
    "Output - Rule 3": ["I", "J"],
    #"Output - Rule 4": ["H", "I"],

}
for sheet_name, cols in custom_deletions.items():
    if sheet_name in wb.sheetnames:
        ws = wb[sheet_name]
        col_nums = sorted([column_index_from_string(c) for c in cols], reverse=True)
        for col_idx in col_nums:
            try:
                ws.delete_cols(col_idx)
                print(f"{sheet_name}: Deleted custom column {col_idx}")
            except Exception as e:
                print(f"{sheet_name}: Failed to delete column {col_idx}: {e}")

print("Applied successfully.")
wb.save(file_path)










You are Chief Financial Officer, reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
Your task is to validate the commentary provided for each financial statement line item against the reported variance, along two dimensions:
•	Completeness 
•	Accuracy.
Follow all instructions precisely and return a structured JSON output conforming to the schema provided at the end.

INPUT FORMAT & NORMALISATION
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
|<id>|<line item>|<difference>|<percentage change>|<comment>|
Before processing:
•	Trim whitespaces from all fields
•	Collapse repeated delimiters
•	Replace new lines inside commentary with commentary
•	Create a lowercase copy of commentary for rule checks, while preserving original text for output
•	If <difference> is blank or missing, treat it as 0

EXAMPLE INPUT
|AI1_000001|Loans and borrowings. |265847823|0|"Loans and borrowing (including current / non-current) decreased by 2.5 bn due to: 2.8 bn bond repayment and 1.4 bn standard debt repayments, partially offset by 1.5 bn loan drawdowns for construction and 0.2 bn FX movements."|


EXECUTION ORDER
Perform steps strictly in this order:
•	Parse and normalise input
•	Existence check
•	Grouping Logic
•	Completeness Evaluation
•	Financial Terminology Engine
•	Accuracy Evaluation
•	Final Status Mapping
•	JSON output

LLM must not change this order

EXISTENCE CHECK
If <comment> is:
o	Blank
o	“N/A”
o	Empty
o	<= 3 characters
Then return
o	Completeness = “Fail”
o	Accuracy = “Fail
o	Final_Status = “Missing”
o	Remarks = “Commentary is not provided”
o	Overall_Justification = “Commentary missing”
Do not evaluate further rules.

GROUPING LOGIC
If a commentary includes phrases indicating group level explanation:
o	“including current / non-current” 
o	“including current and non-current”
o	“combined balance”
o	“overall movement”
o	“current+non-current”
o	“movement across portfolio”
o	“short term and long term”
Apply:
o	Treat commentary as applying to all grouped lines
o	Compute Completeness on the sum of variances of grouped lines
o	Output separate JSON records for each line item
o	Apply same Completeness to all grouped lines unless a child line’s direction contradicts commentary
o	If commentary direction conflicts with any child line’s variance, grouped lines contradict commentary direction, Accuracy = Fail for that line

COMPLETENESS EVALUATION
Completeness checks whether commentary fully explains the variance
A commentary passes Completeness if it satisfies either of the following two conditions:

Qualitative Completeness Rule (Priority 1):
Completeness = Pass if commentary provides a clear, specific, qualitative explanation, such as 
•	FX translation
•	Impairment/ impairment reversal
•	Revaluation movement
•	Acquisition/ disposal
•	One-off or exceptional item
•	Prior-period adjustment
•	Provision release
•	Construction progress
•	Debt restructuring 
Generic phrases (“operational movement”, “general activity”, timing differences” without detail) do not qualify.
If a valid qualitative reason is present, do not apply numeric coverage

Quantitative Completeness Rule (Priority 2):
Apply only when numeric amounts appear in the commentary.
Extract numeric values
Regex
([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
Ignore
•	Years
•	Note references
•	Standalone numbers <=100 without units (unless clearly variance values)

Apply scaling:
•	bn/billion → x1,000,000,000
•	mn/million → x1,000,0000
•	k/thousand → x1,000
Infer sign:
•	repayment, settlement, disposal, release → negative
•	drawdown, addition, accrual, issuance → positive
•	numbers prefixed with “-“ → negative
•	numbers without verbs → assume positive
Coverage:
Compute coverage as:
coverage = round(abs(explained_variance_in_commentary) / abs(reported_difference) * 100, 2)
Completeness = Pass if:
80%<=coverage<=120%
Else → Fail

Mixed Explanations
If commentary includes both qualitative and numeric explanations:
•	Use numeric completeness only if numeric content >5%
•	Otherwise treat as qualitative
Zero/Immaterial Variance:
If abs(difference)<1:
•	Completeness = “Pass”
•	Accuracy = “Inconclusive”
•	Final_Status = “Okay”
•	Remarks = “Variance immaterial”
    Completeness check ends here

DOMINANT DIRECTION
Dominant Direction = the net implied direction of movement inferred from the commentary
Apply this hierarchy:
If numeric values exist
•	Sum all variance related numeric amounts (after applying signs) 
•	If sum >0 → dominant = increase
•	If sum <0 → dominant = decrease
If no numeric values exist but directional verbs appear
•	Count increase vs decrease indicators
•	Greater count = dominant direction
If counts are equal or unclear
•	Dominant = none
•	Accuracy = Fail
Dominant direction that must match the sign of <difference>
If not → Accuracy = Fail

FINANCIAL TERMINOLOGY ENGINE
Apply Financial Terminology Engine before Accuracy.
If any terminology is violated → Accuracy = Fail
Direction Implying Terms
Increase indicators
[“drawdown”, “addition”, “accrual”, “capitalisation”, “issuance”, “origination”, “gain”, “recognition”]
Decrease indicators
[“repayment”, “redemption”, “utilisation”, “settlement”, “write-off”, “write-down”, “release”, “derecognition”, “amortisation”, “depreciation”, “impairment”]
If implied direction contradicts variance sign → Fail

Line-Item Terminology Compatibility
PPE/Intangibles/ROU Assets
Allowed
[“addition”, “capitalisation”, “depreciation”, “impairment”, “write-off”, “disposal”, “revaluation”, “acquisition”, “recognition”]
Forbidden
[“collections”, “repayment”, “drawdown”]

Receivables/Financial Assets
Allowed
[“credit sales”, “collections”, “write-off”, “impairment”, “paid”]
Forbidden
[“depreciation”, “capitalisation”, “accrual”]

Inventory
Allowed
[“purchases”, “COGS”, “write-down”, “obsolescence”, “addition”, “write-off”]
Forbidden
[“depreciation”, “collections”]

Borrowings
Allowed
[“drawdown”, “repayment”, “interest accrual”, “refinancing”, “principal payment”, “interest payment”, “interest rate change”]
Forbidden
[“write-down”, “depreciation”]

Payables/Accruals
Allowed
[“accrual”, “settlement”, “utilisation”, “payment”]
Forbidden
[“depreciation”, “capitalisation”]

Equity/Reserves
Allowed
[“dividend”, “profit retention”, “OCI movement”, “profit for the year”, “reserve”, “foreign currency translation reserve”, “share capital”, “retained earnings”, “shareholder account”, “Owners Equity”]
Forbidden
[“repayment”, “drawdown”]

Revenue
Allowed
[“volume”, “price”, “mix”, “FX”, “contract changes”, “increased customer during the year”]
Forbidden
[“cost optimisation”, “inventory purchases”]

Terminology Misuse 
Fail if commentary uses:
•	Circular logic (“decrease in due to lower balance”)
•	Domain-inappropriate terms (“inventory decreased due to depreciation”)
•	Non-financial drivers (“improved morale”)
•	Vague terms (e.g., operational reasons, general movement, business as usual, standard activity, other adjustments, etc.)
•	Terminology opposing variance direction

Mixed Terminology
If both increase and decrease terms appear
•	If numeric → dominant = net numeric sign
•	If qualitative → dominant must match variance
•	If ambiguous → Fail

ACCURACY EVALUATION
Accuracy checks:
1.	Directional correctness
2.	Reason correctness (substantive, non-vague)
A commentary passes Accuracy if:
Direction Check:
Variance direction = sign of <difference>?
•	positive → increase
•	negative → decrease
Commentary must match or imply direction.
Contradiction → Accuracy = Fail
Reason Check:
Valid reasons include:
•	FX movements
•	Impairment/reversal
•	Revaluation
•	Acquisition/ disposal
•	One-off items
•	Prior period adjustments
•	Provision movements
•	Debt repayment/ drawdown
Fails Accuracy if:
•	Vague
•	Contradictory 
•	Mismatched with variance direction
•	Unrelated to the line item
•	Violates Financial Terminology Engine Rules

Mixed Directions
Follow Hierarchy
•	Numeric → sum sign is dominant
•	Qualitative → must match variance
•	Otherwise → Fail

Accuracy Justification Format
•	Pass: “Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).”
•	Fail: “Commentary direction contradicts variance or lacks substantive reason.”

    

FINAL STATUS MAPPING
Apply exactly
If Existence = Fail → Final_Status = “Missing” AND Remarks = “Commentary is not provided”
Else If Completeness = Fail AND Accuracy = Fail → “Incomplete and Inaccurate”
Else If Completeness = Fail AND Accuracy = Pass → “Incomplete”
Else If Completeness = Pass AND Accuracy = Fail → “Inaccurate”
Else If Accuracy = “Inconclusive” → “Inconclusive”
Else → “Okay”

No additional text outside the JSON array.
    Overall_Justification = Short summary combining completeness and accuracy outcomes (e.g., “Commentary covers direction but not full variance explanation.”)
    
OUTPUT SCHEMA
Return only a JSON array of objects:
[
    {
        "id": "string",
        "line_item": "string",
        "completeness": "Pass" | "Fail",
        "completeness_justification": "string",
        "accuracy": "Pass" | "Fail",
        "accuracy_justification": "string",
        "Final_Status": "OK" | "Incomplete" | "Inaccurate" | "Inconclusive" | "Missing",
        "Remarks": "string",
        "Overall_Justification": "string"
    }
    ]
EXAMPLE OUTPUT
    [
    {
        "id": "AI1_000001",
        "line_item": "Loans and borrowings.",
        "completeness": "Fail",
        "completeness_justification": "(-2.8bn –1.4bn +1.5bn +0.2bn = -1.6bn vs total -2.47bn). Explains 65% of 2.47bn combined variance, which is not with in threshold of 80% and 120%.",
        "accuracy": "Pass",
        "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, drawdowns, FX movements).",
        "Final_Status": "Incomplete",
        "Remarks": "Commentary provided is incomplete",
        "Overall_Justification": "Commentary matches variance direction but does not explain full movement."
    }
    ]

    """







You are Chief Financial Officer, reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
Your task is to validate the commentary provided for each financial statement line item against the reported variance, along two dimensions:
•	Completeness 
•	Accuracy.
Follow all instructions precisely and return a structured JSON output conforming to the schema provided at the end.

INPUT FORMAT & NORMALISATION
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
|<id>|<line item>|<difference>|<percentage change>|<comment>|
Before processing:
•	Trim whitespaces from all fields
•	Collapse repeated delimiters
•	Replace new lines inside commentary with commentary
•	Create a lowercase copy of commentary for rule checks, while preserving original text for output
•	If <difference> is blank or missing, treat it as 0

EXAMPLE INPUT
|AI1_000001|Loans and borrowings. |265847823|0|"Loans and borrowing (including current / non-current) decreased by 2.5 bn due to: 2.8 bn bond repayment and 1.4 bn standard debt repayments, partially offset by 1.5 bn loan drawdowns for construction and 0.2 bn FX movements."|


EXECUTION ORDER
Perform steps strictly in this order:
•	Parse and normalise input
•	Existence check
•	Grouping Logic
•	Completeness Evaluation
•	Financial Terminology Engine
•	Accuracy Evaluation
•	Final Status Mapping
•	JSON output

LLM must not change this order

EXISTENCE CHECK
If <comment> is:
o	Blank
o	“N/A”
o	Empty
o	<= 3 characters
Then return
o	Completeness = “Fail”
o	Accuracy = “Fail
o	Final_Status = “Missing”
o	Remarks = “Commentary is not provided”
o	Overall_Justification = “Commentary missing”
Do not evaluate further rules.

GROUPING LOGIC
If a commentary includes phrases indicating group level explanation:
o	“including current / non-current” 
o	“including current and non-current”
o	“combined balance”
o	“overall movement”
o	“current+non-current”
o	“movement across portfolio”
o	“short term and long term”
Apply:
o	Treat commentary as applying to all grouped lines
o	Compute Completeness on the sum of variances of grouped lines
o	Output separate JSON records for each line item
o	Apply same Completeness to all grouped lines unless a child line’s direction contradicts commentary
o	If commentary direction conflicts with any child line’s variance, grouped lines contradict commentary direction, Accuracy = Fail for that line

COMPLETENESS EVALUATION
Completeness checks whether commentary fully explains the variance
A commentary passes Completeness if it satisfies either of the following two conditions:

Qualitative Completeness  Rule (Priority 1):
Completeness = Pass if commentary provides a clear, specific, qualitative explanation, such as 
•	FX translation
•	Impairment/ impairment reversal
•	Revaluation movement
•	Acquisition/ disposal
•	One-off or exceptional item
•	Prior-period adjustment
•	Provision release
•	Construction progress
•	Debt restructuring 
Generic phrases (“operational movement”, “general activity”, timing differences” without detail) do not qualify.
If a valid qualitative reason is present, do not apply numeric coverage

Quantitative Completeness Rule (Priority 2):
Apply only when numeric amounts appear in the commentary.
Extract numeric values
Regex
([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
Ignore
•	Years
•	Note references
•	Standalone numbers <=100 without units (unless clearly variance values)

Apply scaling:
•	bn/billion → x1,000,000,000
•	mn/million → x1,000,0000
•	k/thousand → x1,000
Infer sign:
•	repayment, settlement, disposal, release → negative
•	drawdown, addition, accrual, issuance → positive
•	numbers prefixed with “-“ → negative
•	numbers without verbs → assume positive
Coverage:
Compute coverage as:
coverage = round(abs(explained_variance_in_commentary) / abs(reported_difference) * 100, 2)
Completeness = Pass if:
80%<=coverage<=120%
Else → Fail

Mixed Explanations
If commentary includes both qualitative and numeric explanations:
•	Use numeric completeness only if numeric content >80%
•	Otherwise treat as qualitative
Zero/Immaterial Variance:
If abs(difference)<1:
•	Completeness = “Pass”
•	Accuracy = “Inconclusive”
•	Final_Status = “Okay”
•	Remarks = “Variance immaterial”
    Completeness check ends here

FINANCIAL TERMINOLOGY ENGINE
Apply Financial Terminology Engine before Accuracy.
If any terminology is violated → Accuracy = Fail
Direction Implying Terms
Increase indicators
[“drawdown”, “addition”, “accrual”, “capitalisation”, “issuance”, “origination”, “gain”, “recognition”]
Decrease indicators
[“repayment”, “redemption”, “utilisation”, “settlement”, “write-off”, “write-down”, “release”, “derecognition”, “amortisation”, “depreciation”, “impairment”]
If implied direction contradicts variance sign → Fail

Line-Item Terminology Compatibility
PPE/Intangibles/ROU Assets
Allowed
[“addition”, “capitalisation”, “depreciation”, “impairment”, “write-off”, “disposal”, “revaluation”]
Forbidden
[“collections”, “repayment”, “drawdown”]

Receivables/Financial Assets
Allowed
[“credit sales”, “collections”, “write-off”, “impairment”]
Forbidden
[“depreciation”, “capitalisation”, “accrual”]

Inventory
Allowed
[“purchases”, “COGS”, “write-down”, “obsolescence”]
Forbidden
[“depreciation”, “collections”]

Borrowings
Allowed
[“drawdown”, “repayment”, “interest accrual”, “refinancing”]
Forbidden
[“write-down”, “depreciation”]

Payables/Accruals
Allowed
[“accrual”, “settlement”, “utilisation”]
Forbidden
[“depreciation”, “capitalisation”]

Equity/Reserves
Allowed
[“dividend”, profit retention”, “OCI movement”]
Forbidden
[“repayment”, “drawdown”]

Revenue
Allowed
[“volume”, “price”, “mix”, “FX”, “contract changes”]
Forbidden
[“cost optimisation”, “inventory purchases”]

Terminology Misuse 
Fail if commentary uses:
•	Circular logic (“decrease in due to lower balance”)
•	Domain-inappropriate terms (“inventory decreased due to depreciation”)
•	Non-financial drivers (“improved morale”)
•	Vague terms (e.g., operational reasons, general movement, business as usual, standard activity, other adjustments, etc.)
•	Terminology opposing variance direction

Mixed Terminology
If both increase and decrease terms appear
•	If numeric → dominant = net numeric sign
•	If qualitative → dominant must match variance
•	If ambiguous → Fail

ACCURACY EVALUATION
Accuracy checks:
1.	Directional correctness
2.	Reason correctness (substantive, non-vague)
A commentary passes Accuracy if:
Direction Check:
Variance direction = sign of <difference?
•	positive → increase
•	negative → decrease
Commentary must match or imply direction.
Contradiction → Accuracy = Fail
Reason Check:
Valid reasons include:
•	FX movements
•	Impairment/reversal
•	Revaluation
•	Acquisition/ disposal
•	One-off items
•	Prior period adjustments
•	Provision movements
•	Debt repayment/ drawdown
Fails Accuracy if:
•	Vague
•	Contradictory 
•	Mismatched with variance direction
•	Unrelated to the line item
•	Violates Financial Terminology Engine Rules

Mixed Directions
Follow Hierarchy
•	Numeric → sum sign is dominant
•	Qualitative → must match variance
•	Otherwise → Fail

Accuracy Justification Format
•	Pass: “Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).”
•	Fail: “Commentary direction contradicts variance or lacks substantive reason.”

    

FINAL STATUS MAPPING
Apply exactly
If Existence = Fail → Final_Status = “Missing” AND Remarks = “Commentary is not provided”
Else If Completeness = Fail AND Accuracy = Fail → “Incomplete and Inaccurate”
Else If Completeness = Fail AND Accuracy = Pass → “Incomplete”
Else If Completeness = Pass AND Accuracy = Fail → “Inaccurate”
Else If Accuracy = “Inconclusive” → “Inconclusive”
Else → “Okay”

No additional text outside the JSON array.
    Overall_Justification = Short summary combining completeness and accuracy outcomes (e.g., “Commentary covers direction but not full variance explanation.”)
    
OUTPUT SCHEMA
Return only a JSON array of objects:
[
    {
        "id": "string",
        "line_item": "string",
        "completeness": "Pass" | "Fail",
        "completeness_justification": "string",
        "accuracy": "Pass" | "Fail",
        "accuracy_justification": "string",
        "Final_Status": "OK" | "Incomplete" | "Inaccurate" | "Inconclusive" | "Missing",
        "Remarks": "string",
        "Overall_Justification": "string"
    }
    ]
EXAMPLE OUTPUT
    [
    {
        "id": "AI1_000001",
        "line_item": "Loans and borrowings.",
        "completeness": "Fail",
        "completeness_justification": "(-2.8bn –1.4bn +1.5bn +0.2bn = -1.6bn vs total -2.47bn). Explains 65% of 2.47bn combined variance, which is not with in threshold of 80% and 120%.",
        "accuracy": "Pass",
        "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, drawdowns, FX movements).",
        "Final_Status": "Incomplete",
        "Remarks": "Commentary provided is incomplete",
        "Overall_Justification": "Commentary matches variance direction but does not explain full movement."
    }
    ]

    """








prompt = """

    You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
    Your task is to validate the commentary provided for each financial statement line item against the reported variance, along two dimensions:
    Completeness and Accuracy.
    Follow all instructions precisely and return a structured JSON output conforming to the schema provided at the end.

    INPUT FORMAT
    Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
    |<id>|<line item>|<difference>|<percentage change>|<comment>|
    If difference is blank or missing, treat it as 0.
    If a grouping variance is referenced, treat the total variance as the sum of the grouped lines.
    GROUPING LOGIC
    If a commentary includes phrases such as “including current / non-current” or “including current and non-current”:
    1.	Treat that commentary as group-level, applying it to all related lines (e.g., “Loans and borrowings.” and “Loans and borrowings – current.”).
    2.	Evaluate completeness based on the combined variance of all grouped lines.
    3.	Still output a separate JSON object per line item.
    4.	Assign identical results across grouped lines unless their variance direction contradicts the commentary.

    COMPLETENESS – Evaluates how fully the commentary explains the variance.
    A commentary passes Completeness if it satisfies either of the following two conditions:

    Qualitative Explanation Rule:
    If the commentary does not include numeric variance amounts, but provides a clear explanation of the variance reason (e.g., FX translation, impairment, revaluation, acquisition, disposal, one-off adjustment, prior-period true-up, provision release), it automatically passes Completeness.
    Examples of valid qualitative reasons:
    “Entire movement explained qualitatively (due to FX revaluation).”
    “Variance caused by one-off prior-period adjustment.”

    Quantitative Coverage Rule:
    If the commentary includes numeric values, extract them using the regex:
    ([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?

    Apply scaling based on units (m/mn/million → ×1,000,000; bn/billion → ×1,000,000,000; k/thousand → ×1,000).

    Compute coverage as:
    coverage = round(abs(explained_variance_in_commentary) / abs(reported_difference) * 100, 2)

    If coverage >= 80% and <=120% → Pass Completeness
    Otherwise → Fail Completeness

    Priority Note:

    Always check for qualitative reasons first.
    Only perform numeric coverage calculation if numeric amounts are present in the commentary.
    Do not mark completeness as Fail solely due to missing numbers if a valid qualitative reason exists.
    Ignore irrelevant numbers:
    o   Years/dates (e.g., 2025, FY2024, Q1 2025)
    o   Small standalone identifiers ≤100 without units (e.g., “16 OSSA”, “Note 37”, “section 5”)

    Completeness Justification Format:

    Qualitative explanation only:
    “Entire movement explained qualitatively (due to FX revaluation and impairment release).”
    Quantitative explanation:
    “(-2.1 bn + 0.3 bn = -1.8 bn vs total -2.1 bn). Explains (abs(-1.8)/abs(-2.1))% of 2.1 bn variance, which is with in threshold of 80% and 120%.”
    Fail example:
    “(-0.9 bn explained, insufficient coverage). Explains (abs(-0.9)/abs(2.1))% of 2.1 bn variance, which is not with threshold of 80% and 120%.”

    Completeness check ends here

    ACCURACY – Evaluates whether the commentary correctly describes the variance direction and reason.
    A commentary passes Accuracy if:
    1.	The stated direction (increase/decrease) matches the sign of the variance, and
    2.	It provides a specific substantive reason (not generic).
    Recognized substantive reasons include (examples):
    •	Reclassification (with context)
    •	FX translation (with rate or currency)
    •	Acquisition or disposal (with name or timing)
    •	Provision release or impairment (with rationale)
    •	One-off or exceptional items (e.g., one-time gain/loss, prior-period adjustment)
    Fails Accuracy if:
    •	Direction contradicts variance sign.
    •	Commentary is vague (e.g., “FX differences”, “reclassified”, "additions", "disposals") without context.
    •	Contains mixed directions with no dominant explanation.

    Accuracy Justification Format
    •	Pass: “Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).”
    •	Fail: “Commentary direction contradicts variance or lacks substantive reason.”

    IMPLEMENTATION NOTES
    •	Trim whitespace from all parsed fields.
    •	Convert all extracted numeric values to floats after applying scale.
    •	Use the sign of difference to determine variance direction (positive = increase, negative = decrease).
    •	Round all monetary values to two decimals.
    •	Ignore numeric coverage for Completeness if no variance-related numbers are present in commentary.
    OUTPUT SCHEMA
    Return only a JSON array of objects:
    [
    {
        "id": "string",
        "line_item": "string",
        "completeness": "Pass" | "Fail",
        "completeness_justification": "string",
        "accuracy": "Pass" | "Fail",
        "accuracy_justification": "string",
        "Final_Status": "OK" | "Incomplete" | "Inaccurate" | "Inconclusive" | "Missing",
        "Remarks": "string",
        "Overall_Justification": "string"
    }
    ]
    FINAL STATUS MAPPING LOGIC
    If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
    Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
    Else if Completeness = Fail and Accuracy = Pass → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
    Else if Accuracy = Fail and Completeness = Pass → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
    Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is inconclusive”
    Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”

    No additional text outside the JSON array.
    Overall_Justification = Short summary combining completeness and accuracy outcomes (e.g., “Commentary covers direction but not full variance explanation.”)
    EXAMPLE INPUT
    |AI1_000001|Loans and borrowings.|265847823|0|"Loans and borrowing (including current / non-current) decreased by 2.5 bn due to: 2.8 bn bond repayment and 1.4 bn standard debt repayments, partially offset by 1.5 bn loan drawdowns for construction and 0.2 bn FX movements."|
    EXAMPLE OUTPUT
    [
    {
        "id": "AI1_000001",
        "line_item": "Loans and borrowings.",
        "completeness": "Fail",
        "completeness_justification": "(-2.8bn –1.4bn +1.5bn +0.2bn = -1.6bn vs total -2.47bn). Explains 65% of 2.47bn combined variance, which is not with in threshold of 80% and 120%.",
        "accuracy": "Pass",
        "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, drawdowns, FX movements).",
        "Final_Status": "Incomplete",
        "Remarks": "Commentary provided is incomplete",
        "Overall_Justification": "Commentary matches variance direction but does not explain full movement."
    }
    ]

    """


























# Your AI system prompt 
prompt = """You are a financial expert reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group. Your task is to reword and enhance commentary for each financial statement line item based on the provided data.

The objective is to produce clear, concise, and professional commentary suitable for management reporting. Improve readability and tone without changing the underlying meaning and without inventing any new information.

INPUT FORMAT:
Each input row follows this pipe-delimited format:
|<line item>|<current>|<prior>|<difference>|<difference percentage>|<comment>|

Notes:
- All currency values are in AED (full amounts, e.g., 136695881666).
- Percentage change may be supplied but must NOT be used in the output.
- Comments may contain both increase and decrease/offset drivers.

OUTPUT FORMAT RULES:

1. **Direction of movement**:
   - If Difference > 0 → “There is an increase of AED <difference in mn> mn…”
   - If Difference < 0 → “There is a decrease of AED <difference in mn> mn…”
   - If Difference = 0 → “There is no change, with the balance remaining flat at AED <current in mn> mn.”

2. **Difference formatting**:
   - Convert the difference to millions (mn), rounded to one decimal.
   - Do not include percentage change in the output.

3. **Reason extraction & classification**:
   - Increase drivers: additions, inflows, recognitions, growth.
   - Decrease/offset drivers: depreciation, amortization, repayments, write-offs, outflows.
   - A “reason” refers to a *primary driver*.  
     Secondary/explanatory statements should be treated as *sub-reasons*.

4. **Bullet rule (based ONLY on count of primary reasons)**:
   - If total **primary** reasons (increase + decrease/offset) are **3 or fewer** →  
     **Use narrative paragraph. No bullets.**
   - If **more than 3 primary reasons** →  
     **Use bullets** (parent reasons only).  
     Sub-reasons may appear as indented sub-bullets (•).

5. **No reasons in comment**:
   - “There is an increase/decrease of AED <difference> mn. No additional commentary provided.”

6. **Narrative formatting** (≤3 primary reasons):
   - Join reasons smoothly using appropriate linking phrases.
   - Incorporate sub-reasons naturally within the narrative.

7. **Bullet formatting** (>3 primary reasons):
   - Only primary reasons receive “–”.
   - Sub-reasons should appear as “•”.

8. **Do not add any new information**.  
   Reword only for clarity and tone.

9. **Plain text output only**.  
   No markdown, no headings, no JSON.

10. **For multiple input rows**, output commentary for each row separately in the same order.

----------------------------------------------------------
EXAMPLES (UPDATED TO MATCH NEW RULES)
----------------------------------------------------------

Example 1  
More than 3 primary reasons → Bullets

Input:
|Property, plant and equipment|20,000,000,000|17,110,000,000|2,890,000,000|17|The increase pertains to significant additions during the year amounting to AED 2.89 billion. Additions mainly include AED 998 m for purchase of Vessel - Electric Star by XYZ whereas remaining additions pertain to WIP and additions from ABC Group. This increase is net off by depreciation charge of AED 520m. Additions in WIP relates to construction of warehouses in LMN and Ports infrastructure.|

Primary reasons identified:
1. Significant additions (parent)
2. Depreciation charge (parent)
*(Sub-reasons do not count toward bullet threshold)*  
But total parent reasons = 2 → Should be narrative?  
**No — in this case, “significant additions” contains multiple distinct sub-components, but is still ONE primary reason.  
Total primary reasons = 2 → narrative.**  

Updated Output:
There is an increase of AED 2,890.0 mn mainly due to significant additions during the year, including AED 998 m for the purchase of the Vessel – Electric Star by XYZ, work-in-progress additions relating to warehouse construction in LMN and port infrastructure, and assets transferred from ABC Group, partly offset by a depreciation charge of AED 520 m.

---

Example 2  
Only one primary reason → Narrative

Input:
|Intangible assets|15,900,000,000|15,400,000,000|500,000,000|3|Increase mainly due to recognition of goodwill from TL acquisition.|

Output:
There is an increase of AED 500.0 mn mainly due to the recognition of goodwill arising from the TL acquisition.

---

Example 3  
Four primary reasons → Bullets

Input:
|Loans and borrowings|55,252,105,504|54,986,791,694|265,313,810|0|2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements.|

Primary reasons:
1. Loan drawdowns  
2. FX & other movements  
3. Bond repayment  
4. Standard debt repayment  
→ 4 primary reasons → use bullets.

Output:
There is an increase of AED 265.3 mn due to the following:
- Loan drawdowns of AED 1.5 bn related to the construction of M2RO and S4RO.
  • Includes AED 0.9 bn under the HQ revolving credit facility.
- Foreign exchange and other movements of AED 0.2 bn.
- Bond repayment of AED 2.8 bn, which offset the increase.
- Standard debt repayments of AED 1.4 bn.

---

Example 4  
Two primary reasons → Narrative

Input:
|Trade receivables|5,000,000,000|5,400,000,000|-400,000,000|-7|Decrease due to higher customer collections and lower billing.|

Output:
There is a decrease of AED 400.0 mn due to higher customer collections and reduced billing during the period.

"""

















print(file_path)

wb = load_workbook(file_path)

# Sheets to process
sheets_to_process = [s for s in wb.sheetnames if s.lower().startswith("output - rule") or s.lower().startswith("output - sdm & tm")]
print(sheets_to_process)

# Styles
header_fill = PatternFill(start_color="151B54", end_color="151B54", fill_type="solid")
header_font = Font(bold=True, color="FFFFFF")
pattern_fill = PatternFill(start_color="ECECEC", end_color="ECECEC", fill_type="solid")
pattern_font = Font(bold=True, color="000000")
green_fill = PatternFill(start_color="93DC5C", end_color="93DC5C", fill_type="solid")
red_fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
soft_delete_fill = PatternFill(start_color="FFFFFF", end_color="FFFFFF", fill_type="solid")
soft_delete_font = Font(color="FFFFFF", italic=True)
thin_border = Border(
    left=Side(style="thin"),
    right=Side(style="thin"),
    top=Side(style="thin"),
    bottom=Side(style="thin")
)
def apply_sdm_tm_format(ws):
    print(f"Applying SDM/TM block: {ws.title}")
    max_row = ws.max_row
    max_col = ws.max_column

    phrase = [
        "all forms have been completed for the entity",
        "all tasks have been completed for the entity",
        "sdm file not found in sharepoint",
        "task manager file not found in sharepoint"
    ]
    row=1
    while row <= max_row:
        
        cell_b = ws.cell(row=row, column=2)  # column B
        val = cell_b.value
        if val and isinstance(val, str):
            clean=val.strip().lower()

            if any(p in clean for p in phrase):
                #Applying bold only
                cell_b.font = Font(bold=True)
                row += 1
                print("made it bold only")
                continue

        if val and isinstance(val, str) and val.strip().lower().startswith("below"):             
                cell_b.font = Font(bold=True)
                header_row_idx = row + 1
                while header_row_idx <= max_row and (ws.cell(row=header_row_idx, column=1).value is None and ws.cell(row=header_row_idx, column=2).value is None):
                    header_row_idx += 1

                if header_row_idx <= max_row:
                    for c in range(1, max_col + 1):
                        hcell = ws.cell(row=header_row_idx, column=c)
                        if hcell.value is not None and str(hcell.value).strip() != "":
                            hcell.fill = header_fill
                            hcell.font = header_font
                            hcell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

                    # applying table borders to rows after header 
                    data_row = header_row_idx + 1
                    while data_row <= max_row:
                        b_val = ws.cell(row=data_row, column=2).value
                        row_blank = all((ws.cell(row=data_row, column=col).value is None or str(ws.cell(row=data_row, column=col).value).strip() == "") for col in range(1, max_col + 1))
                        if (b_val and isinstance(b_val, str) and b_val.strip().lower().startswith("below")) or row_blank:
                            break

                        for c in range(2, max_col + 1):
                            dcell = ws.cell(row=data_row, column=c)
                            dcell.border = Border(left=Side(style="thin"), right=Side(style="thin"),
                                                top=Side(style="thin"), bottom=Side(style="thin"))
                            dcell.alignment = Alignment(wrap_text=True, vertical="top")
                        data_row += 1
                    row = data_row
                    continue            
        row += 1

    print(f"Completed SDM/TM formatting on {ws.title}")

sr_regex = re.compile(r"\bsr\.?\s*no\.?\b", re.I)


def normalize_header_text(s):
    return re.sub(r"[^A-Za-z0-9]", "", str(s or "")).lower()

def soft_delete_cell(cell):
    cell.fill = soft_delete_fill
    cell.font = soft_delete_font
    cell.value = ""

for sheet_name in sheets_to_process:
    ws = wb[sheet_name]
    print(f"Processing sheet: {ws.title}")

    ws.sheet_view.showGridLines = False
    ws.sheet_view.zoomScale = 70
    ws.sheet_properties.tabColor = "081F59"


    # Finding last used row and column
    last_row = 0
    last_col = 0
    for row in ws.iter_rows():
        for cell in row:
            if cell.value not in (None, ""):
                last_row = max(last_row, cell.row)
                last_col = max(last_col, cell.column)

    # Hide unused columns
    for col in range(last_col + 1, ws.max_column + 1):
        ws.column_dimensions[get_column_letter(col)].hidden = True
    # Hide unused rows
    for r in range(last_row + 1, ws.max_row + 1):
        ws.row_dimensions[r].hidden = True

    try:
        for r in [2, 3, 4]:
            if r <= ws.max_row:
                for cell in ws[r]:
                    cell.font = Font(bold=True, size=18)
        if 6 <= ws.max_row:
            for cell in ws[6]:
                cell.font = Font(bold=True)
        if 8 <= ws.max_row:
            for cell in ws[8]:
                cell.font = Font(bold=True)       
    except Exception as e:
        print(f"Row-formatting skipped for {sheet_name}: {e}")

    if "output - sdm" in sheet_name.lower():
        apply_sdm_tm_format(ws)

    # Row scanning
    row = 1
    while row <= last_row:
        raw_texts = [str(c.value).strip() if c.value else "" for c in ws[row]]
        header_norms = [normalize_header_text(txt) for txt in raw_texts]

        # Detect SR No row
        sr_col_idx = next((idx for idx, txt in enumerate(raw_texts) if txt and sr_regex.search(txt)), None)
        if sr_col_idx is not None:
            pattern_row_idx = row - 1

            # Identify helper columns
            helper_cols = {name: header_norms.index(name) for name in ["rownumber", "cellnumber", "sheetname"] if name in header_norms}

            # Pattern row formatting
            first_helper_col = min(helper_cols.values()) if helper_cols else len(header_norms)
            for idx, cell in enumerate(ws[pattern_row_idx][1:], start=1):
                if idx >= first_helper_col:
                    break
                cell.fill = pattern_fill
                cell.font = pattern_font
                left = Side(style="medium") if idx == 1 else Side(style=None)
                right = Side(style="medium") if idx == first_helper_col - 1 else Side(style=None)
                cell.border = Border(top=Side(style="medium"), bottom=Side(style="medium"), left=left, right=right)

            # Header row formatting
            for idx, cell in enumerate(ws[row][1:], start=1):
                if idx in helper_cols.values():
                    continue
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal="center", vertical="center")
                cell.border = thin_border

            # Soft-delete helper header cells
            for helper_col in helper_cols.values():
                if helper_col < len(ws[row]):
                    soft_delete_cell(ws[row][helper_col])

            # Hyperlink-related columns
            link_col_index = helper_cols.get("rownumber") or helper_cols.get("cellnumber")
            header_type = "rownumber" if "rownumber" in helper_cols else "cellnumber" if "cellnumber" in helper_cols else None
            sheetname_col_index = helper_cols.get("sheetname")

            # Identify target columns based on sheet
            notename_col_idx = header_norms.index("notename") if "notename" in header_norms else None
            desc_col_idx = header_norms.index("notenamedescription") if "notenamedescription" in header_norms else None
            account_col_idx = header_norms.index("accountname") if "accountname" in header_norms else None
            remarks_col_idx = header_norms.index("remarks") if "remarks" in header_norms else None

            # TB Value and Amount column indexes
            tb_value_idx = header_norms.index("tbvalue") if "tbvalue" in header_norms else None
            amount_idx = header_norms.index("amount") if "amount" in header_norms else None

            # Define target columns for hyperlinks based on sheet name
            if "output - rule 1" in sheet_name.lower():
                target_columns = [idx for idx in [desc_col_idx] if idx is not None]
            elif "output - rule 2" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 3" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 4" in sheet_name.lower():
                target_columns = [idx for idx in [notename_col_idx] if idx is not None]

                remarks_link_target_sheet = "Output - Rule 4.1"
            else:
                target_columns = []

            # Process data rows
            data_idx = row + 1
            while data_idx <= last_row:
                data_row = ws[data_idx]
                if all((c.value is None or str(c.value).strip() == "") for c in data_row):
                    break

                # Add borders
                for idx, c in enumerate(data_row[1:], start=1):
                    if idx not in helper_cols.values():
                        c.border = thin_border

                # Conditional formatting
                header_texts = [normalize_header_text(c.value) for c in ws[row]]
                cp_idx = header_texts.index("currentperiod") if "currentperiod" in header_texts else None
                vs_idx = header_texts.index("validationstatus") if "validationstatus" in header_texts else None

                for idx in [cp_idx, vs_idx]:
                    if idx is not None and data_row[idx].value:
                        val = str(data_row[idx].value).strip().lower()
                        if val in ["pass", "tb validated"]:
                            data_row[idx].fill = green_fill
                        elif val == "fail":
                            data_row[idx].fill = red_fill

                # --- TB VALUE / AMOUNT FORMATTING ---
                for fmt_idx in [tb_value_idx, amount_idx]:
                    if fmt_idx is not None and fmt_idx < len(data_row):
                        cell = data_row[fmt_idx]
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '#,##0_);(#,##0);'

                # --- HYPERLINK LOGIC (conditional per sheet) ---
                if link_col_index is not None and sheetname_col_index is not None:
                    link_cell_value = ws.cell(row=data_idx, column=link_col_index + 1).value
                    sheetname_value = ws.cell(row=data_idx, column=sheetname_col_index + 1).value

                    if link_cell_value and sheetname_value:
                        safe_sheet_name = str(sheetname_value).strip()
                        if any(ch in safe_sheet_name for ch in [" ", "-", "."]):
                            safe_sheet_name = f"'{safe_sheet_name}'"

                        try:
                            if header_type == "rownumber":
                                target_number = int(link_cell_value)
                                target_ref = f"{safe_sheet_name}!A{target_number}:Z{target_number}"
                            elif header_type == "cellnumber":
                                target_ref = f"{safe_sheet_name}!{str(link_cell_value).upper().strip()}"
                            else:
                                target_ref = None

                            if target_ref:
                                for idx in target_columns:
                                    link_target_cell = data_row[idx]
                                    if link_target_cell and link_target_cell.value:
                                        link_target_cell.hyperlink = f"#{target_ref}"
                                        link_target_cell.font = Font(color="0000FF", underline="single")
                                        # print(f"{sheet_name}: Linked '{link_target_cell.value}' aÃƒÂ¢Ã¢â€šÂ¬Ã‚Â ÃƒÂ¢Ã¢â€šÂ¬Ã¢â€žÂ¢ {target_ref}")
                        except ValueError:
                            pass
                # Hyperlink for Rule 4 Remarks column
                if sheet_name.strip().lower() == "output - rule 4" and remarks_col_idx is not None:
                    if 'Output - Rule 4.1' in wb.sheetnames:
                        try:
                            current_row_number = data_idx  # current row 
                            remarks_cell = data_row[remarks_col_idx]
                            if remarks_cell and remarks_cell.value:
                                target_ref = f"'{remarks_link_target_sheet}'!A{current_row_number}:Z{current_row_number}"
                                remarks_cell.hyperlink = f"#{target_ref}"
                                remarks_cell.font = Font(color="0000FF", underline="single")
                                # print(f"{sheet_name}: Linked Remarks (Row {current_row_number}) Ã¢â€ â€™ {target_ref}")
                        except Exception as e:
                            print(f"{sheet_name}: Failed to link Remarks at row {data_idx} Ã¢â€ â€™ {e}")
                
                # Soft-delete helper columns
                for helper_col in helper_cols.values():
                    if helper_col < len(data_row):
                        soft_delete_cell(data_row[helper_col])

                data_idx += 1

            row = data_idx + 1
            continue
        row += 1
    
    # Autofit columns
    for col in ws.columns:
        try:
            max_length = 0
            col_letter = get_column_letter(col[0].column)
            for cell in col:
                if cell.value is not None:
                    max_length = max(max_length, len(str(cell.value)) + 4) # for changing column width
            ws.column_dimensions[col_letter].width = max(1, max_length)
        except Exception:
            pass

    try:
        
        ws.column_dimensions["A"].width = 2
        if ws.title != "Output - SDM & TM":
            ws.column_dimensions["B"].width = 6
    except Exception:
        pass

    if sheet_name.strip().lower() == "output - rule 4.1":
        print("Applying formatting to Output - Rule 4.1")

        # Adjust column widths
        col_widths = {
            "C": 10,
            "D": 25,  
            "E": 28,
            "F": 25,
            "G": 15,
            "H": 28,  
            "I": 15,
            "J": 28,
            "K": 15,
            "L": 28,
            "M": 28,
            "N": 28,
        }
        skip_rows = [2,3,4,6,8]
        for col_letter, width in col_widths.items():
            ws.column_dimensions[col_letter].width = width
        # Text wrapping 
        for row in ws.iter_rows():
            for cell in row:
                row_num = row[0].row        
                if row_num not in skip_rows:
                    cell.alignment = Alignment(wrap_text=True, vertical="top")

# Custom deletions
custom_deletions = {
    "Output - Rule 1": ["G","H"],
    "Output - Rule 2": ["I", "J"],
    "Output - Rule 3": ["I", "J"],
    #"Output - Rule 4": ["H", "I"],

}
for sheet_name, cols in custom_deletions.items():
    if sheet_name in wb.sheetnames:
        ws = wb[sheet_name]
        col_nums = sorted([column_index_from_string(c) for c in cols], reverse=True)
        for col_idx in col_nums:
            try:
                ws.delete_cols(col_idx)
                print(f"{sheet_name}: Deleted custom column {col_idx}")
            except Exception as e:
                print(f"{sheet_name}: Failed to delete column {col_idx}: {e}")

print("Applied successfully.")
wb.save(file_path)







































COMPLETENESS — coverage of variance

A commentary passes Completeness if it satisfies either of the following two conditions:

Qualitative Explanation Rule:

If the commentary does not include numeric variance amounts, but provides a clear explanation of the variance reason (e.g., FX translation, impairment, revaluation, acquisition, disposal, one-off adjustment, prior-period true-up, provision release), it automatically passes Completeness.

Examples of valid qualitative reasons:

“Entire movement explained qualitatively (due to FX revaluation).”

“Variance caused by one-off prior-period adjustment.”

Quantitative Coverage Rule:

If the commentary includes numeric values, extract them using the regex:
([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?

Apply scaling based on units (m/mn/million → ×1,000,000; bn/billion → ×1,000,000,000; k/thousand → ×1,000).

Compute coverage as:

coverage = round(abs(explained_variance_in_commentary) / abs(reported_difference) * 100, 2)


If coverage >= 80% → Pass Completeness

Otherwise → Fail Completeness

Priority Note:

Always check for qualitative reasons first.

Only perform numeric coverage calculation if numeric amounts are present in the commentary.

Do not mark completeness as Fail solely due to missing numbers if a valid qualitative reason exists.

Completeness Justification Format:

Qualitative explanation only:

“Entire movement explained qualitatively (due to FX revaluation and impairment release).”

Quantitative explanation:

“Explains 85% of 2.5 bn variance. (-2.1 bn + 0.3 bn = -1.8 bn vs total -2.1 bn).”

Fail example:

“Explains 45% of 2.1 bn variance. (-0.9 bn explained, insufficient coverage).”



You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
Your task is to validate the commentary provided for each financial statement line item against the reported variance along two dimensions:
Completeness and Accuracy.
Follow all rules exactly and return a structured JSON output matching the schema at the end.
INPUT FORMAT
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
|<id>|<line item>|<difference>|<percentage change>|<comment>|
•	If variance_for_grouping is blank or missing, treat it as equal to difference.
GROUPING LOGIC
•	If commentary includes “including current / non-current” (or “including current and non-current”):
1.	Treat that commentary as group-level, applying to all related lines (e.g., “Loans and borrowings.”, “Loans and borrowings – current.”).
2.	Use the combined movement of those lines for completeness evaluation.
3.	Return a separate JSON object for each line.
4.	Related lines share the same completeness and accuracy unless variance directions contradict the commentary.
COMPLETENESS — coverage of variance (Updated & Consistent)
A commentary passes Completeness if it satisfies either of the following:
1.	Qualitative Explanation Rule:
o	If the commentary does not include numeric variance amounts, but provides a clear explanation of the variance reason (e.g., FX translation, impairment, revaluation, acquisition, disposal, one-off adjustment, prior-period true-up, provision release), it automatically passes Completeness.
o	Examples:
	“Entire movement explained qualitatively (due to FX revaluation).”
	“Variance caused by one-off prior-period adjustment.”
2.	Quantitative Coverage Rule:
o	If the commentary includes numeric values, extract them using the regex:
o	([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
o	Apply scaling:
	m/mn/million → ×1,000,000
	bn/billion → ×1,000,000,000
	k/thousand → ×1,000
o	Compute coverage:
o	coverage = round(abs(explained_variance_in_commentary) / abs(reported_difference) * 100, 2)
o	Pass Completeness if coverage >= 80%, else Fail.
Priority Note:
•	Always check for qualitative reasons first.
•	Only calculate numeric coverage if numeric values are present.
•	Do not mark Completeness as Fail solely due to missing numbers if a valid qualitative reason exists.
Completeness Justification Format:
•	Qualitative only:
“Entire movement explained qualitatively (due to FX revaluation and impairment release).”
•	Quantitative:
“Explains 85% of 2.5 bn variance. (-2.1 bn + 0.3 bn = -1.8 bn vs total -2.1 bn).”
•	Fail example:
“Explains 45% of 2.1 bn variance. (-0.9 bn explained, insufficient coverage).”
ACCURACY — correctness of direction and reason
•	Numeric validation is not required.
•	Evaluate only direction and reason.
Pass if:
1.	Commentary direction (increase/decrease) matches variance sign.
2.	Commentary provides a specific substantive reason (not generic).
Recognized substantive reasons include (non-exhaustive):
•	Reclassification (with context)
•	FX translation (with rate or currency)
•	Acquisition / disposal (with name / timing)
•	Provision release (with rationale)
•	One-off / exceptional items: “one-off favourable adjustment”, “exceptional gain/loss”, “non-recurring adjustment”, “prior-period true-up or correction”
Fail if:
•	Direction contradicts variance
•	Commentary is vague (“FX differences”, “reclassified”) without context
•	Mixed increase/decrease with no dominant direction
Accuracy Justification Format:
•	Pass:
“Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).”
•	Fail:
“Commentary direction contradicts variance or lacks substantive reason.”
IMPLEMENTATION NOTES
•	Trim whitespace in parsed fields.
•	Convert extracted numbers to float after scaling.
•	Use sign of difference to determine direction.
•	Round monetary values to two decimals in JSON.
FINAL STATUS MAPPING LOGIC
•	If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
•	Else If Completeness = Fail and Accuracy = Fail → Final_Status = "Incomplete and Inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
•	Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
•	Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
•	Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is inconclusive”
•	Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”
OUTPUT SCHEMA
Return a JSON array of objects:
[
  {
    "id": "string", 
    "line_item": "string",
    "completeness": "Pass" | "Fail",
    "completeness_justification": "string",
    "accuracy": "Pass" | "Fail",
    "accuracy_justification": "string",
    "Final_Status": "OK | Incomplete | Inaccurate | Inconclusive",
    "Remarks": "Commentary provided is valid | Commentary provided is inaccurate and incomplete | Commentary provided is incomplete | Commentary provided is inaccurate",
    "Overall_Justification": "string"
  }
]
EXAMPLE INPUT
|AI1_000001|Loans and borrowings.|265847823|0|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of ABC and DEF including 0.9 BN HQ XYZ. 0.2 BN FX & other movements."|
|AI1_000030|Loans and borrowings - current.|-2732813238|-30|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements."|
EXAMPLE OUTPUT
[
  {
    "id": "AI1_000001",  
    "line_item": "Loans and borrowings.",
    "completeness": "Fail",
    "completeness_justification": "Explains 65% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → below threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, loan drawdowns, FX movements).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": "Commentary does not explain full variance, but direction and reason are correct."
  }
]








###Prompt New###



You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
Your task is to validate the commentary provided for each financial statement line item against the reported variance, along two dimensions:
Completeness and Accuracy.
Follow all instructions precisely and return a structured JSON output conforming to the schema provided at the end.
INPUT FORMAT
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
|<id>|<line item>|<difference>|<percentage change>|<comment>|
If difference is blank or missing, treat it as 0.
If a grouping variance is referenced, treat the total variance as the sum of the grouped lines.
GROUPING LOGIC
If a commentary includes phrases such as “including current / non-current” or “including current and non-current”:
1.	Treat that commentary as group-level, applying it to all related lines (e.g., “Loans and borrowings.” and “Loans and borrowings – current.”).
2.	Evaluate completeness based on the combined variance of all grouped lines.
3.	Still output a separate JSON object per line item.
4.	Assign identical results across grouped lines unless their variance direction contradicts the commentary.
COMPLETENESS – Evaluates how fully the commentary explains the variance.
A commentary passes Completeness if either:
•	It provides a qualitative explanation of the movement (even without numeric details), or
•	The numeric explanation accounts for ≥ 80 % of the absolute variance amount.
Step 1: Numeric Extraction
Extract numeric values and interpret their scale and direction.
Regex:
([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
Direction keywords (within 3 words):
•	Positive: increase, higher, gain, up, +
•	Negative: decrease, lower, decline, down, −
Scaling:
•	m / mn / million → × 1,000,000
•	bn / billion → × 1,000,000,000
•	k / thousand → × 1,000
Ignore numbers that are clearly years (e.g., 2025) or small identifiers (≤ 100 without units).
Step 2: Coverage Calculation
Sum all extracted numeric amounts (with direction and scaling).
coverage = round(abs(explained_variance) ÷ abs(variance_from_input) × 100, 2)
If no numeric data is found, treat the commentary as qualitative and automatically pass Completeness, provided the text contains a descriptive cause (e.g., “due to FX revaluation”).
Completeness Justification Format
Explains <coverage>% of <variance> variance. (<numeric breakdown>)
Examples:
•	“Explains 100 % of 2.01 bn variance. (5.2 bn + 0.5 bn + 0.2 bn – 3.9 bn = 2.0 bn)”
•	“Entire movement explained qualitatively (due to FX revaluation).”

ACCURACY – Evaluates whether the commentary correctly describes the variance direction and reason.
A commentary passes Accuracy if:
1.	The stated direction (increase/decrease) matches the sign of the variance, and
2.	It provides a specific substantive reason (not generic).
Recognized substantive reasons include (examples):
•	Reclassification (with context)
•	FX translation (with rate or currency)
•	Acquisition or disposal (with name or timing)
•	Provision release or impairment (with rationale)
•	One-off or exceptional items (e.g., one-time gain/loss, prior-period adjustment)
Fails Accuracy if:
•	Direction contradicts variance sign.
•	Commentary is vague (e.g., “FX differences”, “reclassified”) without context.
•	Contains mixed directions with no dominant explanation.

Accuracy Justification Format
•	Pass: “Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).”
•	Fail: “Commentary direction contradicts variance or lacks substantive reason.”

IMPLEMENTATION NOTES
•	Trim whitespace from all parsed fields.
•	Convert all extracted numeric values to floats after applying scale.
•	Use the sign of difference to determine variance direction (positive = increase, negative = decrease).
•	Round all monetary values to two decimals.
•	Ignore numeric coverage for Completeness if no variance-related numbers are present in commentary.
OUTPUT SCHEMA
Return only a JSON array of objects:
[
  {
    "id": "string",
    "line_item": "string",
    "completeness": "Pass" | "Fail",
    "completeness_justification": "string",
    "accuracy": "Pass" | "Fail",
    "accuracy_justification": "string",
    "Final_Status": "OK" | "Incomplete" | "Inaccurate" | "Inconclusive" | "Missing",
    "Remarks": "string",
    "Overall_Justification": "string"
  }
]
FINAL STATUS MAPPING LOGIC
If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is Inconclusive”
Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”
No additional text outside the JSON array.
Overall_Justification = Short summary combining completeness and accuracy outcomes (e.g., “Commentary covers direction but not full variance explanation.”)
EXAMPLE INPUT
|AI1_000001|Loans and borrowings.|265847823|0|"Loans and borrowing (including current / non-current) decreased by 2.5 bn due to: 2.8 bn bond repayment and 1.4 bn standard debt repayments, partially offset by 1.5 bn loan drawdowns for construction and 0.2 bn FX movements."|
EXAMPLE OUTPUT
[
  {
    "id": "AI1_000001",
    "line_item": "Loans and borrowings.",
    "completeness": "Fail",
    "completeness_justification": "Explains 65% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.2bn = -1.6bn vs total -2.47bn).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, drawdowns, FX movements).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": "Commentary matches variance direction but does not explain full movement."
  }
]





You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
 
Your task is to validate the commentary provided for each financial statement line item against the reported variance along two dimensions:
 
Completeness and Accuracy.
 
Follow all rules exactly and return a structured JSON output matching the schema at the end.
 
INPUT FORMAT
 
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
 
|<id>|<line item>|<difference>|<percentage change>|<comment>|  
 
If variance_for_grouping is blank or missing, treat it as equal to difference.
 
GROUPING LOGIC
 
If commentary includes “including current / non-current” (or “including current and non-current”), then:
 
1.  Treat that commentary as group-level, applying to all related lines (e.g., “Loans and borrowings.”, “Loans and borrowings – current.”).
 
2.  Use the combined movement of those lines for completeness evaluation.
 
3.  Still return a separate JSON object for each line.
 
4.  Related lines share the same completeness and accuracy unless their variance direction contradicts the commentary.
 
COMPLETENESS — coverage of variance

A commentary passes Completeness if there is a qualitative explanation without any mention of variance amount.
A commentary passes Completeness if it explains ≥ 80 % of the absolute variance amount.

Step 1: Numeric extraction
 
Extract numeric values and apply sign/scale:
 
•   Regex: ([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
 
•   Direction keywords within 3 words:
 
Positive: increase, higher, gain, up, +
 
Negative: decrease, lower, decline, down, −
 
•   Apply scaling:
 
m / mn / million → × 1 000 000
 
bn / billion → × 1 000 000 000
 
k / thousand → × 1 000
 
•   Ignore years (e.g., 2025) or small identifiers (≤ 100 w/o units).
 
Step 2: Coverage calculation
 
Sum all valid numbers (with sign + scale).
 
coverage = round(abs(explained variance in commentary) ÷ abs(absolute variance provided as difference in input) × 100, 2)
 
If no numeric data:
 
•   Accept qualitative explanations, no matter the coverage.

Completeness justification format
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)

Explains <coverage>% of <variance> variance. (<numeric breakdown>)
 
Examples:
 
•   Explains 100 % of 2.01 bn variance. (5.2 bn + 0.5 bn + 0.2 bn – 3.9 bn = 2.0 bn)
 
•   Explains 52.5 % of 162.9 m variance. (29.5 m + 26 m = 55.5 m explained)
 
•   Entire movement explained qualitatively (due to FX revaluation).
 
ACCURACY — correctness of direction and reason
 
For Accuracy, numeric validation is not required.
 
Evaluate only direction and reason.
 
Pass if:
 
1.  Commentary direction (increase/decrease) matches variance sign.
 
2.  Commentary gives a specific substantive reason (not a generic mechanism).
 
Recognized substantive reasons include (non-exhaustive):
 
•   Reclassification (with context)
 
•   FX translation (with rate or currency)
 
•   Acquisition / disposal (with name / timing)
 
•   Provision release (with rationale)
 
•   One-off or exceptional items:
 
“one-off favourable adjustment”, “exceptional gain/loss”, “non-recurring adjustment”, “one-time impairment or release”, “prior-period true-up or correction”
 
Fail if:
 
•   Direction contradicts variance.
 
•   Commentary is vague (“FX differences”, “reclassified”) without context.
 
•   Mixed increase / decrease with no dominant direction.
 
Accuracy justification format
 
Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).
 
Commentary direction contradicts variance or lacks substantive reason.
 
IMPLEMENTATION NOTES
 
•   Trim whitespace in parsed fields.
 
•   Convert extracted numbers to float after scaling.
 
•   Use sign of difference to determine direction.
 
•   Round monetary values to two decimals in the JSON.

IMPORTANT

Do not consider any numeric coverage for completeness if there are no variance amounts mentioned in the commentary.


OUTPUT SCHEMA
 
Return a JSON array of objects:
 
{
  "id": "string", 
  "line_item": "string",
  "completeness": "Pass" | "Fail",
  "completeness_justification": "string",
  "accuracy": "Pass" | "Fail",
  "accuracy_justification": "string",
  "Final_Status": "OK | Incomplete | Inaccurate | Inconclusive",
  "Remarks": "Commentary provided is valid | Commentary provided is inaccurate and incomplete | Commentary provided is incomplete | Commentary provided is inaccurate",
  "Overall_Justification": "<short summary combining the 2 checks>"

}


FINAL STATUS MAPPING LOGIC
If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is Inconclusive”
Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”


No additional text outside the JSON array.
 
EXAMPLE INPUT
 
|AI1_000001|Loans and borrowings.|265847823|0|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of ABC and DEF including 0.9 BN HQ XYZ. 0.2 BN FX & other movements."|
 
|AI1_000030|Loans and borrowings - current.|-2732813238|-30|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements."|

EXAMPLE OUTPUT
 
[
 
  {
    "id": "AI1_000001",  
    "line_item": "Loans and borrowings.",
    "completeness": "Fail",
    "completeness_justification": "Explains 65% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, not acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, loan drawdowns, FX movements).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": Commentary provided does not explain the variance, but matches the direction of movement"

  },
 
  {
    "id": "AI1_000030",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Fail",
    "completeness_justification": "Explains 65% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, not acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, drawdowns, FX adjustments).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": Commentary provided does not explain the variance, but matches the direction of movement"

  },

  {
    "id": "AI1_000040",
    "line_item": "Accounts and other receivables",
    "completeness": "Fail",
    "completeness_justification": "Explains .3% of 1.10bn variance. (-57m - 10m - 11m + 45m = -33m explained, which is below threshold, and thus not acceptable.",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches negative variance and provides substantive reasons (deferred costs, returns, amortization, interest receivable).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": "Commentary provides a qualitative explanation matching the direction and reason for the variance."
  },

 
]

"""



print(file_path)

wb = load_workbook(file_path)

# Sheets to process
sheets_to_process = [s for s in wb.sheetnames if s.lower().startswith("output - rule")]
print(sheets_to_process)

# Styles
header_fill = PatternFill(start_color="151B54", end_color="151B54", fill_type="solid")
header_font = Font(bold=True, color="FFFFFF")
pattern_fill = PatternFill(start_color="ECECEC", end_color="ECECEC", fill_type="solid")
pattern_font = Font(bold=True, color="000000")
green_fill = PatternFill(start_color="93DC5C", end_color="93DC5C", fill_type="solid")
red_fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
soft_delete_fill = PatternFill(start_color="FFFFFF", end_color="FFFFFF", fill_type="solid")
soft_delete_font = Font(color="FFFFFF", italic=True)
thin_border = Border(
    left=Side(style="thin"),
    right=Side(style="thin"),
    top=Side(style="thin"),
    bottom=Side(style="thin")
)

sr_regex = re.compile(r"\bsr\.?\s*no\.?\b", re.I)

def normalize_header_text(s):
    return re.sub(r"[^A-Za-z0-9]", "", str(s or "")).lower()

def soft_delete_cell(cell):
    cell.fill = soft_delete_fill
    cell.font = soft_delete_font
    cell.value = ""

for sheet_name in sheets_to_process:
    ws = wb[sheet_name]
    print(f"Processing sheet: {ws.title}")

    ws.sheet_view.showGridLines = False
    ws.sheet_view.zoomScale = 70
    ws.sheet_properties.tabColor = "081F59"


    # Finding last used row and column
    last_row = 0
    last_col = 0
    for row in ws.iter_rows():
        for cell in row:
            if cell.value not in (None, ""):
                last_row = max(last_row, cell.row)
                last_col = max(last_col, cell.column)

    # Hide unused columns
    for col in range(last_col + 1, ws.max_column + 1):
        ws.column_dimensions[get_column_letter(col)].hidden = True
    # Hide unused rows
    for r in range(last_row + 1, ws.max_row + 1):
        ws.row_dimensions[r].hidden = True

    try:
        for r in [2, 3, 4]:
            if r <= ws.max_row:
                for cell in ws[r]:
                    cell.font = Font(bold=True, size=18)
        if 6 <= ws.max_row:
            for cell in ws[6]:
                cell.font = Font(bold=True)
        if 8 <= ws.max_row:
            for cell in ws[8]:
                cell.font = Font(bold=True)       
    except Exception as e:
        print(f"Row-formatting skipped for {sheet_name}: {e}")

    # Row scanning
    row = 1
    while row <= last_row:
        raw_texts = [str(c.value).strip() if c.value else "" for c in ws[row]]
        header_norms = [normalize_header_text(txt) for txt in raw_texts]

        # Detect SR No row
        sr_col_idx = next((idx for idx, txt in enumerate(raw_texts) if txt and sr_regex.search(txt)), None)
        if sr_col_idx is not None:
            pattern_row_idx = row - 1

            # Identify helper columns
            helper_cols = {name: header_norms.index(name) for name in ["rownumber", "cellnumber", "sheetname"] if name in header_norms}

            # Pattern row formatting
            first_helper_col = min(helper_cols.values()) if helper_cols else len(header_norms)
            for idx, cell in enumerate(ws[pattern_row_idx][1:], start=1):
                if idx >= first_helper_col:
                    break
                cell.fill = pattern_fill
                cell.font = pattern_font
                left = Side(style="medium") if idx == 1 else Side(style=None)
                right = Side(style="medium") if idx == first_helper_col - 1 else Side(style=None)
                cell.border = Border(top=Side(style="medium"), bottom=Side(style="medium"), left=left, right=right)

            # Header row formatting
            for idx, cell in enumerate(ws[row][1:], start=1):
                if idx in helper_cols.values():
                    continue
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal="center", vertical="center")
                cell.border = thin_border

            # Soft-delete helper header cells
            for helper_col in helper_cols.values():
                if helper_col < len(ws[row]):
                    soft_delete_cell(ws[row][helper_col])

            # Hyperlink-related columns
            link_col_index = helper_cols.get("rownumber") or helper_cols.get("cellnumber")
            header_type = "rownumber" if "rownumber" in helper_cols else "cellnumber" if "cellnumber" in helper_cols else None
            sheetname_col_index = helper_cols.get("sheetname")

            # Identify target columns based on sheet
            notename_col_idx = header_norms.index("notename") if "notename" in header_norms else None
            desc_col_idx = header_norms.index("notenamedescription") if "notenamedescription" in header_norms else None
            account_col_idx = header_norms.index("accountname") if "accountname" in header_norms else None
            remarks_col_idx = header_norms.index("remarks") if "remarks" in header_norms else None

            # TB Value and Amount column indexes
            tb_value_idx = header_norms.index("tbvalue") if "tbvalue" in header_norms else None
            amount_idx = header_norms.index("amount") if "amount" in header_norms else None

            # Define target columns for hyperlinks based on sheet name
            if "output - rule 1" in sheet_name.lower():
                target_columns = [idx for idx in [desc_col_idx] if idx is not None]
            elif "output - rule 2" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 3" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 4" in sheet_name.lower():
                target_columns = [idx for idx in [notename_col_idx] if idx is not None]

                remarks_link_target_sheet = "Output - Rule 4.1"
            else:
                target_columns = []

            # Process data rows
            data_idx = row + 1
            while data_idx <= last_row:
                data_row = ws[data_idx]
                if all((c.value is None or str(c.value).strip() == "") for c in data_row):
                    break

                # Add borders
                for idx, c in enumerate(data_row[1:], start=1):
                    if idx not in helper_cols.values():
                        c.border = thin_border

                # Conditional formatting
                header_texts = [normalize_header_text(c.value) for c in ws[row]]
                cp_idx = header_texts.index("currentperiod") if "currentperiod" in header_texts else None
                vs_idx = header_texts.index("validationstatus") if "validationstatus" in header_texts else None

                for idx in [cp_idx, vs_idx]:
                    if idx is not None and data_row[idx].value:
                        val = str(data_row[idx].value).strip().lower()
                        if val in ["pass", "tb validated"]:
                            data_row[idx].fill = green_fill
                        elif val == "fail":
                            data_row[idx].fill = red_fill

                # --- TB VALUE / AMOUNT FORMATTING ---
                for fmt_idx in [tb_value_idx, amount_idx]:
                    if fmt_idx is not None and fmt_idx < len(data_row):
                        cell = data_row[fmt_idx]
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '#,##0_);(#,##0);'

                # --- HYPERLINK LOGIC (conditional per sheet) ---
                if link_col_index is not None and sheetname_col_index is not None:
                    link_cell_value = ws.cell(row=data_idx, column=link_col_index + 1).value
                    sheetname_value = ws.cell(row=data_idx, column=sheetname_col_index + 1).value

                    if link_cell_value and sheetname_value:
                        safe_sheet_name = str(sheetname_value).strip()
                        if any(ch in safe_sheet_name for ch in [" ", "-", "."]):
                            safe_sheet_name = f"'{safe_sheet_name}'"

                        try:
                            if header_type == "rownumber":
                                target_number = int(link_cell_value)
                                target_ref = f"{safe_sheet_name}!A{target_number}:Z{target_number}"
                            elif header_type == "cellnumber":
                                target_ref = f"{safe_sheet_name}!{str(link_cell_value).upper().strip()}"
                            else:
                                target_ref = None

                            if target_ref:
                                for idx in target_columns:
                                    link_target_cell = data_row[idx]
                                    if link_target_cell and link_target_cell.value:
                                        link_target_cell.hyperlink = f"#{target_ref}"
                                        link_target_cell.font = Font(color="0000FF", underline="single")
                                        # print(f"{sheet_name}: Linked '{link_target_cell.value}' aÃ¢â‚¬Â Ã¢â‚¬â„¢ {target_ref}")
                        except ValueError:
                            pass
                # Hyperlink for Rule 4 Remarks column
                if sheet_name.strip().lower() == "output - rule 4" and remarks_col_idx is not None:
                    if 'Output - Rule 4.1' in wb.sheetnames:
                        try:
                            current_row_number = data_idx  # current row 
                            remarks_cell = data_row[remarks_col_idx]
                            if remarks_cell and remarks_cell.value:
                                target_ref = f"'{remarks_link_target_sheet}'!A{current_row_number}:Z{current_row_number}"
                                remarks_cell.hyperlink = f"#{target_ref}"
                                remarks_cell.font = Font(color="0000FF", underline="single")
                                # print(f"{sheet_name}: Linked Remarks (Row {current_row_number}) â†’ {target_ref}")
                        except Exception as e:
                            print(f"{sheet_name}: Failed to link Remarks at row {data_idx} â†’ {e}")

                # Soft-delete helper columns
                for helper_col in helper_cols.values():
                    if helper_col < len(data_row):
                        soft_delete_cell(data_row[helper_col])

                data_idx += 1

            row = data_idx + 1
            continue
        row += 1
    
    # Autofit columns
    for col in ws.columns:
        try:
            max_length = 0
            col_letter = get_column_letter(col[0].column)
            for cell in col:
                if cell.value is not None:
                    max_length = max(max_length, len(str(cell.value)) + 4) # for changing column width
            ws.column_dimensions[col_letter].width = max(1, max_length)
        except Exception:
            pass

    try:
        ws.column_dimensions["A"].width = 2
        ws.column_dimensions["B"].width = 6
    except Exception:
        pass

    if sheet_name.strip().lower() == "output - rule 4.1":
        print("Applying formatting to Output - Rule 4.1")

        # Adjust column widths
        col_widths = {
            "C": 10,
            "D": 25,  
            "E": 28,
            "F": 25,
            "G": 15,
            "H": 28,  
            "I": 15,
            "J": 28,
            "K": 15,
            "L": 28,
            "M": 28,
            "N": 28,
        }
        skip_rows = [2,3,4,6,8]
        for col_letter, width in col_widths.items():
            ws.column_dimensions[col_letter].width = width
        # Text wrapping 
        for row in ws.iter_rows():
            for cell in row:
                row_num = row[0].row        
                if row_num not in skip_rows:
                    cell.alignment = Alignment(wrap_text=True, vertical="top")

# Custom deletions
custom_deletions = {
    "Output - Rule 1": ["G","H"],
    "Output - Rule 2": ["I", "J"],
    "Output - Rule 3": ["I", "J"],
    #"Output - Rule 4": ["H", "I"],

}
for sheet_name, cols in custom_deletions.items():
    if sheet_name in wb.sheetnames:
        ws = wb[sheet_name]
        col_nums = sorted([column_index_from_string(c) for c in cols], reverse=True)
        for col_idx in col_nums:
            try:
                ws.delete_cols(col_idx)
                print(f"{sheet_name}: Deleted custom column {col_idx}")
            except Exception as e:
                print(f"{sheet_name}: Failed to delete column {col_idx}: {e}")

print("Applied successfully.")
wb.save(file_path)












































prompt = """

You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
 
Your task is to validate the commentary provided for each financial statement line item against the reported variance along two dimensions:
 
Completeness and Accuracy.
 
Follow all rules exactly and return a structured JSON output matching the schema at the end.
 
INPUT FORMAT
 
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
 
|<id>|<line item>|<difference>|<percentage change>|<variance_for_grouping>|<comment>|  
 
If variance_for_grouping is blank or missing, treat it as equal to difference.
 
GROUPING LOGIC
 
If commentary includes “including current / non-current” (or “including current and non-current”), then:
 
1.  Treat that commentary as group-level, applying to all related lines (e.g., “Loans and borrowings.”, “Loans and borrowings – current.”).
 
2.  Use the combined movement of those lines for completeness evaluation.
 
3.  Still return a separate JSON object for each line.
 
4.  Related lines share the same completeness and accuracy unless their variance direction contradicts the commentary.
 
COMPLETENESS — coverage of variance
 
A commentary passes Completeness if it explains ≥ 80 % of the absolute variance amount.
Step 1: Numeric extraction
 
Extract numeric values and apply sign/scale:
 
•   Regex: ([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
 
•   Direction keywords within 3 words:
 
Positive: increase, higher, gain, up, +
 
Negative: decrease, lower, decline, down, −
 
•   Apply scaling:
 
m / mn / million → × 1 000 000
 
bn / billion → × 1 000 000 000
 
k / thousand → × 1 000
 
•   Ignore years (e.g., 2025) or small identifiers (≤ 100 w/o units).
 
Step 2: Coverage calculation
 
Sum all valid numbers (with sign + scale).
 
coverage = explained variance ÷ absolute variance × 100 %
 
Step 3: Qualitative completeness

If not a single numeric data in entire commentary related to variance:
 
•   Accept qualitative explanations
•   Otherwise → Fail.

Completeness justification format
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)

Explains <coverage>% of <variance> variance. (<numeric breakdown>)
 
Examples:
 
•   Explains 100 % of 2.01 bn variance. (5.2 bn + 0.5 bn + 0.2 bn – 3.9 bn = 2.0 bn)
 
•   Explains 52.5 % of 162.9 m variance. (29.5 m + 26 m = 55.5 m explained)
 
•   Entire movement explained qualitatively (due to FX revaluation).
 
ACCURACY — correctness of direction and reason
 
For Accuracy, numeric validation is not required.
 
Evaluate only direction and reason.
 
Pass if:
 
1.  Commentary direction (increase/decrease) matches variance sign.
 
2.  Commentary gives a specific substantive reason (not a generic mechanism).
 
Recognized substantive reasons include (non-exhaustive):
 
•   Reclassification (with context)
 
•   FX translation (with rate or currency)
 
•   Acquisition / disposal (with name / timing)
 
•   Provision release (with rationale)
 
•   One-off or exceptional items:
 
“one-off favourable adjustment”, “exceptional gain/loss”, “non-recurring adjustment”, “one-time impairment or release”, “prior-period true-up or correction”
 
Fail if:
 
•   Direction contradicts variance.
 
•   Commentary is vague (“FX differences”, “reclassified”) without context.
 
•   Mixed increase / decrease with no dominant direction.
 
Accuracy justification format
 
Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).
 
Commentary direction contradicts variance or lacks substantive reason.
 
IMPLEMENTATION NOTES
 
•   Trim whitespace in parsed fields.
 
•   Convert extracted numbers to float after scaling.
 
•   Use sign of difference to determine direction.
 
•   Round monetary values to two decimals in the JSON.
 
OUTPUT SCHEMA
 
Return a JSON array of objects:
 
{
  "id": "string", 
  "line_item": "string",
  "completeness": "Pass" | "Fail",
  "completeness_justification": "string",
  "accuracy": "Pass" | "Fail",
  "accuracy_justification": "string",
  "Final_Status": "OK | Incomplete | Inaccurate | Inconclusive",
  "Remarks": "Commentary provided is valid | Commentary provided is inaccurate and incomplete | Commentary provided is incomplete | Commentary provided is inaccurate",
  "Overall_Justification": "<short summary combining the 2 checks>"

}


FINAL STATUS MAPPING LOGIC
If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is Inconclusive”
Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”


No additional text outside the JSON array.
 
EXAMPLE INPUT
 
|AI1_000001|Loans and borrowings.|-|265847823|0|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of ABC and DEF including 0.9 BN HQ XYZ. 0.2 BN FX & other movements."|
 
|AI1_000030|Loans and borrowings - current.|-|-2732813238|-30|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements."|
 
EXAMPLE OUTPUT
 
[
 
  {
    "id": "AI1_000001",  
    "line_item": "Loans and borrowings.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, loan drawdowns, FX movements).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  },
 
  {
    "id": "AI1_000030",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, drawdowns, FX adjustments).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  }
 
]

"""


input_table = """
|ID||Line Item|Account|Difference|% Change|Commentary|
ai034176|Accounts and other receivables - current.|-|-1.10328724775E9|-18.51942889573246|Decrease from Deferred financial costs of AED 57m for transaction fees utilized mainly from conversion of EBL loans to Equity.  There is also a decrease in KEPCO AED 10m due to returns of borrowed materials and other receivable & prepayments AED11m due to amortization.  These are partly offset by increase in interest receivable from FD amounting to AED 45m.

"""








[
  {
    "id": "ai034176",
    "line_item": "Accounts and other receivables - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 74.16% of 1,103.29m variance. (-57m -10m -11m +45m = -33m explained; however, the commentary provides qualitative reasons for the remaining movement, which is acceptable for completeness as per rules.)",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (deferred financial costs, returns of borrowed materials, amortization, and interest receivable).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": "Commentary provides both numeric and qualitative explanations for the decrease, matching the direction and giving substantive reasons."
  }
]

If no numeric data is found, pass completeness if commentary provides qualitative explanation.
If numeric data exists but covers < 80%, fail completeness.




prompt = """

You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
 
Your task is to validate the commentary provided for each financial statement line item against the reported variance along two dimensions:
 
Completeness and Accuracy.
 
Follow all rules exactly and return a structured JSON output matching the schema at the end.
 
INPUT FORMAT
 
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
 
|<id>|<line item>|<difference>|<percentage change>|<variance_for_grouping>|<comment>|  
 
If variance_for_grouping is blank or missing, treat it as equal to difference.
 
GROUPING LOGIC
 
If commentary includes “including current / non-current” (or “including current and non-current”), then:
 
1.  Treat that commentary as group-level, applying to all related lines (e.g., “Loans and borrowings.”, “Loans and borrowings – current.”).
 
2.  Use the combined movement of those lines for completeness evaluation.
 
3.  Still return a separate JSON object for each line.
 
4.  Related lines share the same completeness and accuracy unless their variance direction contradicts the commentary.
 
COMPLETENESS — coverage of variance
 
A commentary passes Completeness if it explains ≥ 80 % of the absolute variance amount. If there is no explanation of variance amount and commentary provides a qualitative explanation, commentary passes completeness.
 
Step 1: Numeric extraction
 
Extract numeric values and apply sign/scale:
 
•   Regex: ([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
 
•   Direction keywords within 3 words:
 
Positive: increase, higher, gain, up, +
 
Negative: decrease, lower, decline, down, −
 
•   Apply scaling:
 
m / mn / million → × 1 000 000
 
bn / billion → × 1 000 000 000
 
k / thousand → × 1 000
 
•   Ignore years (e.g., 2025) or small identifiers (≤ 100 w/o units).
 
Step 2: Coverage calculation
 
Sum all valid numbers (with sign + scale).
 
coverage = explained variance ÷ absolute variance × 100 %
 
Step 3: Qualitative completeness
 
If no numeric data related to coverage in entire commentary:
 
•   Accept qualitative explanations even if there is no quantification.
 
•   Otherwise → Fail.
 
Completeness justification format
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)

Explains 100% of <variance> variance.
 
Examples:
 
•   Explains 100 % of 2.01 bn variance. (5.2 bn + 0.5 bn + 0.2 bn – 3.9 bn = 2.0 bn)
 
•   Explains 52.5 % of 162.9 m variance. (29.5 m + 26 m = 55.5 m explained)
 
•   Entire movement explained qualitatively (Since there is no numeric breakdown provided in the commentary).
 
ACCURACY — correctness of direction and reason
 
For Accuracy, numeric validation is not required.
 
Evaluate only direction and reason.
 
Pass if:
 
1.  Commentary direction (increase/decrease) matches variance sign.
 
2.  Commentary gives a specific substantive reason (not a generic mechanism).
 
Recognized substantive reasons include (non-exhaustive):
 
•   Reclassification (with context)
 
•   FX translation (with rate or currency)
 
•   Acquisition / disposal (with name / timing)
 
•   Provision release (with rationale)
 
•   One-off or exceptional items:
 
“one-off favourable adjustment”, “exceptional gain/loss”, “non-recurring adjustment”, “one-time impairment or release”, “prior-period true-up or correction”
 
Fail if:
 
•   Direction contradicts variance.
 
•   Commentary is vague (“FX differences”, “reclassified”) without context.
 
•   Mixed increase / decrease with no dominant direction.
 
Accuracy justification format
 
Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).
 
Commentary direction contradicts variance or lacks substantive reason.
 
IMPLEMENTATION NOTES
 
•   Trim whitespace in parsed fields.
 
•   Convert extracted numbers to float after scaling.
 
•   Use sign of difference to determine direction.
 
•   Round monetary values to two decimals in the JSON.
 
OUTPUT SCHEMA
 
Return a JSON array of objects:
 
{
  "id": "string", 
  "line_item": "string",
  "completeness": "Pass" | "Fail",
  "completeness_justification": "string",
  "accuracy": "Pass" | "Fail",
  "accuracy_justification": "string",
  "Final_Status": "OK | Incomplete | Inaccurate | Inconclusive",
  "Remarks": "Commentary provided is valid | Commentary provided is inaccurate and incomplete | Commentary provided is incomplete | Commentary provided is inaccurate",
  "Overall_Justification": "<short summary combining the 2 checks>"

}


FINAL STATUS MAPPING LOGIC
If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is inconclusive”
Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”


No additional text outside the JSON array.
 
EXAMPLE INPUT
 
|AI1_000001|Loans and borrowings.|265313810|0|265847823|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of ABC and DEF including 0.9 BN HQ XYZ. 0.2 BN FX & other movements."|
 
|AI1_000030|Loans and borrowings - current.|2732813238|-30|-2732813238|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements."|

|AI1_000031|Loans and borrowings - current.|2732813238|-30|-2732813238|"Due to Bond repayment."|

EXAMPLE OUTPUT
 
[
 
  {
    "id": "AI1_000001",  
    "line_item": "Loans and borrowings.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, loan drawdowns, FX movements).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  },
  {
    "id": "AI1_000030",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, drawdowns, FX adjustments).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  },
  {
    "id": "AI1_000031",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance as there is no percentage mentioned in commentary.",
    "accuracy": "Fail",
    "accuracy_justification": "There is no direction provided in the commentary that states increase or decrease",
    "Final_Status": "Fail",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance, but does not mention the direction of movement"

  }



]

"""








Assets,Total Non-current Assets,Total Current assets,Total Assets.,Equity and liabilities,Total Equity.,Liabilities,Total Non-current Liabilities,Total Current liabilities,Total Liabilities.,Total equity and liabilities.,SoFP Validation_Commentary,Operating Profit.,Profit before income tax.,Continuing Operations Income.,Profit for the year.,Profit/(loss) attributable to:,SoPL Validation_Commentary,Total Opening Balance,Movement during the period:,Closing Balance,Total Movement,Total Movement Changes,Validation,Total Movements,Total Movement ,Opening Balance,Total comprehensive income of the year,Items that will not be reclassified to profit or loss,Items that are or may be reclassified subsequently to profit or loss,Transactions with the Owner ,Closing balance as per Note,Validation - comparative period,Non-current ,GN 28 - Payable to project companies,GN 28 - Other Non-Current Liabilities,At period end,Current,GN 28 - Accounts payable due to third parties.,GN 28 - Accrued expenses,Related parties payables - current,GN 28 - Accounts payable due to related parties.,GN 28 - Retention payables.,Current Tax liabilities,As per SoFP,Non-current & Current total














with pd.ExcelWriter(
    file_path,
    engine="openpyxl",
    mode="a",                    # append mode (don’t overwrite)
    if_sheet_exists="overlay"    # allows writing into existing sheet
) as writer:
    df.to_excel(
        writer,
        sheet_name=sheet_name,
        startrow=startrow,       # choose where to start
        startcol=0,              # or any specific column
        index=False,
        header=False
    )






# Create a writer and attach the workbook
writer = pd.ExcelWriter(file_path, engine='openpyxl')
writer.book = workbook
writer.sheets = {ws.title: ws for ws in workbook.worksheets}

# Example: write DataFrame starting at row 10, column 3 (zero-indexed → Excel row 11, column D)
df.to_excel(
    writer,
    sheet_name=sheet_name,
    startrow=10,
    startcol=3,
    index=False,
    header=False
)

# Save changes
writer.close()




with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
    writer.book = book
    writer.sheets = {ws.title: ws for ws in book.worksheets}

    # Write DataFrame to a specific sheet and start position
    # startrow and startcol are zero-indexed
    df.to_excel(
        writer,
        sheet_name="Sheet1",   # Existing sheet name
        startrow=5,            # Write starting at row 6 (since 0-indexed)
        startcol=2,            # Write starting at column C
        index=False,
        header=False           # Set to True if you want to include headers
    )







import fsspec
from pathlib import Path

# Connect to OneLake (Azure Data Lake / OneLake)
fs = fsspec.filesystem(
    "abfs",  # Azure Blob / ADLS filesystem
    account_name="your_account_name",
    account_key="your_account_key"  # or use SAS token / credentials
)

oneLake_folder = "your-container-name/path/to/folder"

# Recursively find all CSV files
csv_files = [
    file for file in fs.find(oneLake_folder) if file.endswith(".csv")
]

# Print filenames only (without full path)
for file_path in csv_files:
    print(Path(file_path).name)




# Python standard libraries
import os, json, csv, glob, re, hashlib
from math import ceil
from pathlib import Path
from typing import List
from functools import reduce

# PySpark
from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, concat, trim, regexp_extract, when, abs, isnan, lower
from pyspark.sql.types import StringType, DoubleType

# Pandas / Excel (do not reinstall)
import pandas as pd
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows

# OpenAI (make sure version is >=1.0.0)
from openai import OpenAI  # or AzureOpenAI if using Azure





import pandas as pd
from pyspark.sql import SparkSession
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, concat, trim, regexp_extract, when, abs
from pyspark.sql.types import StringType, DoubleType
import os, json, fsspec, csv, glob
from math import ceil
from openai import AzureOpenAI
import re
import hashlib
from pathlib import Path
from typing import List
from openai import OpenAI
from pyspark.sql.functions import isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
from pyspark.sql.functions import trim, lower






final_df = final_df.withColumn(
        "Existence_Justification",
        F.when(
            (col("PassFailFlag") == "Fail") & (col("Existence_Status") == "Fail"),
            F.lit("Commentary was not provided")
        ).otherwise(col("Existence_Justification"))
    )


    header_row = 1
    final_header_col = None
    for col in range(1, col_ref_sheet.max_column + 1):
        header_val = col_ref_sheet.cell(row=header_row, column=col).value
        if header_val and str(header_val).strip().lower() == "final_column_header":
            final_header_col = col
            break
 
    # If found, clean each value under it
    if final_header_col:
        for row in range(2, col_ref_sheet.max_row + 1):
            val = col_ref_sheet.cell(row=row, column=final_header_col).value
            if val:
                val = str(val)
                val = re.sub(r"\(", "_", val)  
                val = re.sub(r"\)", "", val)  
                val = re.sub(r"_+", "_", val)  
                val = val.strip("_ ")
                col_ref_sheet.cell(row=row, column=final_header_col, value=val)







from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
import re
 
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    try:
        return float(val)
    except:
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status
 
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            return None
    except:
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary", "row_no"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    row_no_val = df.filter(F.lower(F.col("fsli")) == "commentary").select("row_no").first()["row_no"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val, row_no_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id", "row_no"
    ])
 
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        # "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname = row["sheetname"]
        account = row["account"]
        diff_col = row["differenceheader"]
        pc_col = row["changeheader"]
        comm_col = row["commentaryheader"]
        mappingtableid = row["id"]
 
        path = sheet_paths.get(sheetname)
        if not path:
            continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except:
                cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]):
            continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn(
            "PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id"),
            col("row_no").alias("row_no")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
 
combined_df = reduce(
    DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]
) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window))
 
    try:
        existing_df = spark.read.format("delta").load(final_output_path)
        max_id_row = (
            existing_df.select("id")
            .rdd.map(lambda r: int(re.sub(r"\D", "", r["id"])) if r["id"] else 0)
            .max()
        )
        start_id = max_id_row + 1
        print(f"existing max ID: {max_id_row}")
    except Exception:
        start_id = 1
        print("No existing table found, starting from ID 1.")
 
    final_df = final_df.withColumn(
        "id",
        F.concat(F.lit("AI1_"), F.format_string("%06d", (col("row_num") + F.lit(start_id - 1))))
    ).drop("row_num")
 
    cols = ["id", "row_no"] + [c for c in final_df.columns if c not in ["id", "row_no"]]
    final_df.select(*cols) \
        .write.format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(final_output_path)
 
    print("Final output written successfully ")
else:
    print("No valid data found to write.")




























# final all sheet + gn_18
 
 
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
 
# Config
base_path =  "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
 
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
# Utility functions
def safe_float(val):
    try: return float(val)
    except: return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0: return None
    except: return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
        path = sheet_paths.get(sheetname)
        if not path: continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except: cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]): continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn("PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                          .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                          .drop("row_num")
 
    cols = ["id"] + [c for c in final_df.columns if c != "id"]
    final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
    print(" Final output written successfully .")
else:
    print(" No valid data found to write.")
 


















# final all sheet + gn_18 [with try and logs]
 
 
 
import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Rule4Validation")
 
# Initialize Spark session
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
# Config
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    if val is None or val == "":
        return 0.0
    try:
        return float(val)
    except Exception as e:
        logger.warning(f"Unexpected value for float conversion: {val} — {e}")
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    try:
        if diff_val > 10_000_000:
            flag = "Fail"
        elif pc_val > 10 and diff_val >= 5_000_000:
            flag = "Fail"
        else:
            flag = "Pass"
        existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
        return flag, existence_status
    except Exception as e:
        logger.error(f"Validation error: {e}")
        return "Error", "Error"
 
# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            logger.warning("No GN1811 data found for txnID.")
            return None
    except Exception as e:
        logger.error(f"Error loading GN1811 sheet: {e}")
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        try:
            row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
            if row_df.count() > 0:
                rows[label] = row_df.collect()[0].asDict()
        except Exception as e:
            logger.warning(f"Error extracting {label} row: {e}")
 
    try:
        id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    except Exception as e:
        logger.error(f"Error extracting id from GN1811: {e}")
        id_val = "unknown"
 
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
    result = []
 
    for col_name in asset_cols:
        try:
            diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
            pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
            comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
            flag, existence_status = validate_row(diff_val, pc_val, comm_val)
            result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
        except Exception as e:
            logger.error(f"Error processing column {col_name}: {e}")
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    try:
        mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    except Exception as e:
        logger.error(f"Error loading mapping sheet: {e}")
        return None
 
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        try:
            sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
            path = sheet_paths.get(sheetname)
            if not path:
                logger.warning(f"No path found for sheet: {sheetname}")
                continue
 
            if sheetname not in cached_sheets:
                try:
                    df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                    cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
                except Exception as e:
                    logger.error(f"Error loading sheet {sheetname}: {e}")
                    cached_sheets[sheetname] = None
            df = cached_sheets.get(sheetname)
            if df is None:
                logger.warning(f"Sheet {sheetname}  has no data Corresponding to the given txnID .")
                continue
 
            missing_cols = [c for c in [diff_col, pc_col, comm_col] if c not in df.columns]
            if missing_cols:
                logger.warning(f"Sheet {sheetname} is missing required columns: {missing_cols}")
                continue
 
            if df.filter(col("txnID") == txn_id).count() == 0:
                logger.info(f"Sheet {sheetname} has no data for txnID: {txn_id}")
                continue
 
            df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
            fail_1 = F.abs(col(diff_col)) > 10_000_000
            fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
            df_casted = df_casted.withColumn("PassFailFlag",
                F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
                 .when(fail_1 | fail_2, "Fail")
                 .otherwise("Pass")
            )
 
            formatted = df_casted.select(
                F.lit(sheetname).alias("sheetname"),
                F.lit(mappingtableid).alias("mappingtableid"),
                col("fsli"),
                F.lit(account).alias("account"),
                col(diff_col).alias("difference"),
                col(pc_col).alias("pc_change"),
                col(comm_col).alias("commentary"),
                col("PassFailFlag"),
                F.when(
                    (col("PassFailFlag") == "Fail") &
                    (col(comm_col).isNotNull()) &
                    (~isnan(col(comm_col))) &
                    (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                    "Pass"
                ).otherwise("Fail").alias("Existence_Status"),
                F.lit("").alias("Existence_Justification"),
                F.lit(txn_id).alias("txnID"),
                col("id").alias("odsid"),
                col("id").alias("id")
            )
 
            final_dfs.append(formatted)
        except Exception as e:
            logger.error(f"Error processing mapping row: {e}")
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
try:
    gn1811_df = process_gn1811()
    mapping_df = process_mapping_sheets()
    combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
    if combined_df:
        window = Window.orderBy(F.monotonically_increasing_id())
        final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                              .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                              .drop("row_num")
 
        cols = ["id"] + [c for c in final_df.columns if c != "id"]
        final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
        logger.info(" Final output written successfully.")
    else:
        logger.warning(" No valid data found to write.")
except Exception as e:
    logger.critical(f"Unhandled error during final write: {e}")
 
























F.when(
    (F.col("PassFailFlag") == "Fail") &
    (F.col(comm_col).isNotNull()) &
    (~isnan(F.col(comm_col))) &
    (~F.trim(F.col(comm_col)).isin("", "0", "0.0")),
    "Pass"
).otherwise("Fail").alias("Existence_Status"),

F.lit("").alias("Existence_Justification")


�� ADDH Intercompany dividend payable

joined_df = joined_df.withColumn(
    "fsli",
    F.regexp_replace(F.regexp_replace(F.regexp_replace("fsli", r"[^\x00-\x7F]", ""), r"\\s+", " "), r"^\\s+|\\s+$", "")
)


from openpyxl import load_workbook
from openpyxl.workbook.calc_props import CalcProperties

def enable_auto_calc(file_in, file_out):
    wb = load_workbook(file_in)

    # Check if _calcPr exists, if not, create one
    if not hasattr(wb, '_calcPr') or wb._calcPr is None:
        wb._calcPr = CalcProperties(calcMode='auto', fullCalcOnLoad=True)
    else:
        wb._calcPr.calcMode = 'auto'
        wb._calcPr.fullCalcOnLoad = True

    wb.save(file_out)
    print(f"Saved workbook with automatic calculation: {file_out}")

# Example usage
enable_auto_calc('input.xlsx', 'output_auto_calc.xlsx')




import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Find workbookPr element and update calcMode attribute
    # Excel namespace
    ns = {'ns': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}
    workbookPr = root.find('ns:workbookPr', ns)
    
    if workbookPr is None:
        # If not found, create it
        workbookPr = etree.Element('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}workbookPr')
        root.insert(0, workbookPr)
    
    # Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 5. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 6. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                # archive name should be relative to temp_dir
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 7. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage example
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Extract the namespace from root tag, e.g. '{namespace}workbook'
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    ns = {'ns': ns_uri}
    
    # 5. Find workbookPr element with the correct namespace
    workbookPr = root.find('ns:workbookPr', namespaces=ns)
    
    if workbookPr is None:
        # Create workbookPr if it doesn't exist
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)
    
    # 6. Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 7. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 8. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 9. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')





import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # Clean temp dir if it exists
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract the .xlsx contents
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Path to workbook.xml
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')

    # Step 3: Parse workbook.xml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()

    # Step 4: Extract the namespace dynamically
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 5: Find or create <workbookPr>
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 6: Set or update calcMode to "auto"
    workbookPr.set('calcMode', 'auto')

    # Optional: Remove manual-related flags that force manual behavior
    for attr in ['fullCalcOnLoad', 'forceFullCalc']:
        if attr in workbookPr.attrib:
            del workbookPr.attrib[attr]

    # Step 7: Save the modified workbook.xml
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 8: Repackage everything into a new .xlsx file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)

    # Step 9: Cleanup
    shutil.rmtree(temp_dir)

# ✅ Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'

    # Clean up and create temp dir
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Modify workbook.xml
    workbook_xml = os.path.join(temp_dir, 'xl', 'workbook.xml')
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml, parser)
    root = tree.getroot()

    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 3: Find/create workbookPr
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 4: Set calcMode and cleanup other attributes
    workbookPr.attrib.clear()  # clear all attributes
    workbookPr.set('calcMode', 'auto')

    # Optional: Add Excel default calcId (can help with Excel Online)
    workbookPr.set('calcId', '122211')  # example value from new Excel files

    # Step 5: Write back
    tree.write(workbook_xml, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 6: Re-zip to output_path
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for root_dir, dirs, files in os.walk(temp_dir):
            for file in files:
                abs_path = os.path.join(root_dir, file)
                rel_path = os.path.relpath(abs_path, temp_dir)
                zip_out.write(abs_path, rel_path)

    # Step 7: Cleanup
    shutil.rmtree(temp_dir)

# Example usage (paths must point to accessible Lakehouse or OneLake locations)
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')
