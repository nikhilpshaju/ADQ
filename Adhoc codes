You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
 
Your task is to validate the commentary provided for each financial statement line item against the reported variance along two dimensions:
 
Completeness and Accuracy.
 
Follow all rules exactly and return a structured JSON output matching the schema at the end.
 
INPUT FORMAT
 
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
 
|<id>|<line item>|<difference>|<percentage change>|<comment>|  
 
If variance_for_grouping is blank or missing, treat it as equal to difference.
 
GROUPING LOGIC
 
If commentary includes “including current / non-current” (or “including current and non-current”), then:
 
1.  Treat that commentary as group-level, applying to all related lines (e.g., “Loans and borrowings.”, “Loans and borrowings – current.”).
 
2.  Use the combined movement of those lines for completeness evaluation.
 
3.  Still return a separate JSON object for each line.
 
4.  Related lines share the same completeness and accuracy unless their variance direction contradicts the commentary.
 
COMPLETENESS — coverage of variance

A commentary passes Completeness if there is a qualitative explanation without any mention of variance amount.
A commentary passes Completeness if it explains ≥ 80 % of the absolute variance amount.

Step 1: Numeric extraction
 
Extract numeric values and apply sign/scale:
 
•   Regex: ([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
 
•   Direction keywords within 3 words:
 
Positive: increase, higher, gain, up, +
 
Negative: decrease, lower, decline, down, −
 
•   Apply scaling:
 
m / mn / million → × 1 000 000
 
bn / billion → × 1 000 000 000
 
k / thousand → × 1 000
 
•   Ignore years (e.g., 2025) or small identifiers (≤ 100 w/o units).
 
Step 2: Coverage calculation
 
Sum all valid numbers (with sign + scale).
 
coverage = round(abs(explained variance in commentary) ÷ abs(absolute variance provided as difference in input) × 100, 2)
 
If no numeric data:
 
•   Accept qualitative explanations, no matter the coverage.

Completeness justification format
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)

Explains <coverage>% of <variance> variance. (<numeric breakdown>)
 
Examples:
 
•   Explains 100 % of 2.01 bn variance. (5.2 bn + 0.5 bn + 0.2 bn – 3.9 bn = 2.0 bn)
 
•   Explains 52.5 % of 162.9 m variance. (29.5 m + 26 m = 55.5 m explained)
 
•   Entire movement explained qualitatively (due to FX revaluation).
 
ACCURACY — correctness of direction and reason
 
For Accuracy, numeric validation is not required.
 
Evaluate only direction and reason.
 
Pass if:
 
1.  Commentary direction (increase/decrease) matches variance sign.
 
2.  Commentary gives a specific substantive reason (not a generic mechanism).
 
Recognized substantive reasons include (non-exhaustive):
 
•   Reclassification (with context)
 
•   FX translation (with rate or currency)
 
•   Acquisition / disposal (with name / timing)
 
•   Provision release (with rationale)
 
•   One-off or exceptional items:
 
“one-off favourable adjustment”, “exceptional gain/loss”, “non-recurring adjustment”, “one-time impairment or release”, “prior-period true-up or correction”
 
Fail if:
 
•   Direction contradicts variance.
 
•   Commentary is vague (“FX differences”, “reclassified”) without context.
 
•   Mixed increase / decrease with no dominant direction.
 
Accuracy justification format
 
Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).
 
Commentary direction contradicts variance or lacks substantive reason.
 
IMPLEMENTATION NOTES
 
•   Trim whitespace in parsed fields.
 
•   Convert extracted numbers to float after scaling.
 
•   Use sign of difference to determine direction.
 
•   Round monetary values to two decimals in the JSON.

IMPORTANT

Do not consider any numeric coverage for completeness if there are no variance amounts mentioned in the commentary.


OUTPUT SCHEMA
 
Return a JSON array of objects:
 
{
  "id": "string", 
  "line_item": "string",
  "completeness": "Pass" | "Fail",
  "completeness_justification": "string",
  "accuracy": "Pass" | "Fail",
  "accuracy_justification": "string",
  "Final_Status": "OK | Incomplete | Inaccurate | Inconclusive",
  "Remarks": "Commentary provided is valid | Commentary provided is inaccurate and incomplete | Commentary provided is incomplete | Commentary provided is inaccurate",
  "Overall_Justification": "<short summary combining the 2 checks>"

}


FINAL STATUS MAPPING LOGIC
If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is Inconclusive”
Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”


No additional text outside the JSON array.
 
EXAMPLE INPUT
 
|AI1_000001|Loans and borrowings.|265847823|0|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of ABC and DEF including 0.9 BN HQ XYZ. 0.2 BN FX & other movements."|
 
|AI1_000030|Loans and borrowings - current.|-2732813238|-30|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements."|

EXAMPLE OUTPUT
 
[
 
  {
    "id": "AI1_000001",  
    "line_item": "Loans and borrowings.",
    "completeness": "Fail",
    "completeness_justification": "Explains 65% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, not acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, loan drawdowns, FX movements).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": Commentary provided does not explain the variance, but matches the direction of movement"

  },
 
  {
    "id": "AI1_000030",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Fail",
    "completeness_justification": "Explains 65% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, not acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, drawdowns, FX adjustments).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": Commentary provided does not explain the variance, but matches the direction of movement"

  },

  {
    "id": "AI1_000040",
    "line_item": "Accounts and other receivables",
    "completeness": "Fail",
    "completeness_justification": "Explains .3% of 1.10bn variance. (-57m - 10m - 11m + 45m = -33m explained, which is below threshold, and thus not acceptable.",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches negative variance and provides substantive reasons (deferred costs, returns, amortization, interest receivable).",
    "Final_Status": "Incomplete",
    "Remarks": "Commentary provided is incomplete",
    "Overall_Justification": "Commentary provides a qualitative explanation matching the direction and reason for the variance."
  },

 
]

"""



print(file_path)

wb = load_workbook(file_path)

# Sheets to process
sheets_to_process = [s for s in wb.sheetnames if s.lower().startswith("output - rule")]
print(sheets_to_process)

# Styles
header_fill = PatternFill(start_color="151B54", end_color="151B54", fill_type="solid")
header_font = Font(bold=True, color="FFFFFF")
pattern_fill = PatternFill(start_color="ECECEC", end_color="ECECEC", fill_type="solid")
pattern_font = Font(bold=True, color="000000")
green_fill = PatternFill(start_color="93DC5C", end_color="93DC5C", fill_type="solid")
red_fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")
soft_delete_fill = PatternFill(start_color="FFFFFF", end_color="FFFFFF", fill_type="solid")
soft_delete_font = Font(color="FFFFFF", italic=True)
thin_border = Border(
    left=Side(style="thin"),
    right=Side(style="thin"),
    top=Side(style="thin"),
    bottom=Side(style="thin")
)

sr_regex = re.compile(r"\bsr\.?\s*no\.?\b", re.I)

def normalize_header_text(s):
    return re.sub(r"[^A-Za-z0-9]", "", str(s or "")).lower()

def soft_delete_cell(cell):
    cell.fill = soft_delete_fill
    cell.font = soft_delete_font
    cell.value = ""

for sheet_name in sheets_to_process:
    ws = wb[sheet_name]
    print(f"Processing sheet: {ws.title}")

    ws.sheet_view.showGridLines = False
    ws.sheet_view.zoomScale = 70
    ws.sheet_properties.tabColor = "081F59"


    # Finding last used row and column
    last_row = 0
    last_col = 0
    for row in ws.iter_rows():
        for cell in row:
            if cell.value not in (None, ""):
                last_row = max(last_row, cell.row)
                last_col = max(last_col, cell.column)

    # Hide unused columns
    for col in range(last_col + 1, ws.max_column + 1):
        ws.column_dimensions[get_column_letter(col)].hidden = True
    # Hide unused rows
    for r in range(last_row + 1, ws.max_row + 1):
        ws.row_dimensions[r].hidden = True

    try:
        for r in [2, 3, 4]:
            if r <= ws.max_row:
                for cell in ws[r]:
                    cell.font = Font(bold=True, size=18)
        if 6 <= ws.max_row:
            for cell in ws[6]:
                cell.font = Font(bold=True)
        if 8 <= ws.max_row:
            for cell in ws[8]:
                cell.font = Font(bold=True)       
    except Exception as e:
        print(f"Row-formatting skipped for {sheet_name}: {e}")

    # Row scanning
    row = 1
    while row <= last_row:
        raw_texts = [str(c.value).strip() if c.value else "" for c in ws[row]]
        header_norms = [normalize_header_text(txt) for txt in raw_texts]

        # Detect SR No row
        sr_col_idx = next((idx for idx, txt in enumerate(raw_texts) if txt and sr_regex.search(txt)), None)
        if sr_col_idx is not None:
            pattern_row_idx = row - 1

            # Identify helper columns
            helper_cols = {name: header_norms.index(name) for name in ["rownumber", "cellnumber", "sheetname"] if name in header_norms}

            # Pattern row formatting
            first_helper_col = min(helper_cols.values()) if helper_cols else len(header_norms)
            for idx, cell in enumerate(ws[pattern_row_idx][1:], start=1):
                if idx >= first_helper_col:
                    break
                cell.fill = pattern_fill
                cell.font = pattern_font
                left = Side(style="medium") if idx == 1 else Side(style=None)
                right = Side(style="medium") if idx == first_helper_col - 1 else Side(style=None)
                cell.border = Border(top=Side(style="medium"), bottom=Side(style="medium"), left=left, right=right)

            # Header row formatting
            for idx, cell in enumerate(ws[row][1:], start=1):
                if idx in helper_cols.values():
                    continue
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal="center", vertical="center")
                cell.border = thin_border

            # Soft-delete helper header cells
            for helper_col in helper_cols.values():
                if helper_col < len(ws[row]):
                    soft_delete_cell(ws[row][helper_col])

            # Hyperlink-related columns
            link_col_index = helper_cols.get("rownumber") or helper_cols.get("cellnumber")
            header_type = "rownumber" if "rownumber" in helper_cols else "cellnumber" if "cellnumber" in helper_cols else None
            sheetname_col_index = helper_cols.get("sheetname")

            # Identify target columns based on sheet
            notename_col_idx = header_norms.index("notename") if "notename" in header_norms else None
            desc_col_idx = header_norms.index("notenamedescription") if "notenamedescription" in header_norms else None
            account_col_idx = header_norms.index("accountname") if "accountname" in header_norms else None
            remarks_col_idx = header_norms.index("remarks") if "remarks" in header_norms else None

            # TB Value and Amount column indexes
            tb_value_idx = header_norms.index("tbvalue") if "tbvalue" in header_norms else None
            amount_idx = header_norms.index("amount") if "amount" in header_norms else None

            # Define target columns for hyperlinks based on sheet name
            if "output - rule 1" in sheet_name.lower():
                target_columns = [idx for idx in [desc_col_idx] if idx is not None]
            elif "output - rule 2" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 3" in sheet_name.lower():
                target_columns = [idx for idx in [account_col_idx, notename_col_idx] if idx is not None]
            elif "output - rule 4" in sheet_name.lower():
                target_columns = [idx for idx in [notename_col_idx] if idx is not None]

                remarks_link_target_sheet = "Output - Rule 4.1"
            else:
                target_columns = []

            # Process data rows
            data_idx = row + 1
            while data_idx <= last_row:
                data_row = ws[data_idx]
                if all((c.value is None or str(c.value).strip() == "") for c in data_row):
                    break

                # Add borders
                for idx, c in enumerate(data_row[1:], start=1):
                    if idx not in helper_cols.values():
                        c.border = thin_border

                # Conditional formatting
                header_texts = [normalize_header_text(c.value) for c in ws[row]]
                cp_idx = header_texts.index("currentperiod") if "currentperiod" in header_texts else None
                vs_idx = header_texts.index("validationstatus") if "validationstatus" in header_texts else None

                for idx in [cp_idx, vs_idx]:
                    if idx is not None and data_row[idx].value:
                        val = str(data_row[idx].value).strip().lower()
                        if val in ["pass", "tb validated"]:
                            data_row[idx].fill = green_fill
                        elif val == "fail":
                            data_row[idx].fill = red_fill

                # --- TB VALUE / AMOUNT FORMATTING ---
                for fmt_idx in [tb_value_idx, amount_idx]:
                    if fmt_idx is not None and fmt_idx < len(data_row):
                        cell = data_row[fmt_idx]
                        if isinstance(cell.value, (int, float)):
                            cell.number_format = '#,##0_);(#,##0);'

                # --- HYPERLINK LOGIC (conditional per sheet) ---
                if link_col_index is not None and sheetname_col_index is not None:
                    link_cell_value = ws.cell(row=data_idx, column=link_col_index + 1).value
                    sheetname_value = ws.cell(row=data_idx, column=sheetname_col_index + 1).value

                    if link_cell_value and sheetname_value:
                        safe_sheet_name = str(sheetname_value).strip()
                        if any(ch in safe_sheet_name for ch in [" ", "-", "."]):
                            safe_sheet_name = f"'{safe_sheet_name}'"

                        try:
                            if header_type == "rownumber":
                                target_number = int(link_cell_value)
                                target_ref = f"{safe_sheet_name}!A{target_number}:Z{target_number}"
                            elif header_type == "cellnumber":
                                target_ref = f"{safe_sheet_name}!{str(link_cell_value).upper().strip()}"
                            else:
                                target_ref = None

                            if target_ref:
                                for idx in target_columns:
                                    link_target_cell = data_row[idx]
                                    if link_target_cell and link_target_cell.value:
                                        link_target_cell.hyperlink = f"#{target_ref}"
                                        link_target_cell.font = Font(color="0000FF", underline="single")
                                        # print(f"{sheet_name}: Linked '{link_target_cell.value}' aÃ¢â‚¬Â Ã¢â‚¬â„¢ {target_ref}")
                        except ValueError:
                            pass
                # Hyperlink for Rule 4 Remarks column
                if sheet_name.strip().lower() == "output - rule 4" and remarks_col_idx is not None:
                    if 'Output - Rule 4.1' in wb.sheetnames:
                        try:
                            current_row_number = data_idx  # current row 
                            remarks_cell = data_row[remarks_col_idx]
                            if remarks_cell and remarks_cell.value:
                                target_ref = f"'{remarks_link_target_sheet}'!A{current_row_number}:Z{current_row_number}"
                                remarks_cell.hyperlink = f"#{target_ref}"
                                remarks_cell.font = Font(color="0000FF", underline="single")
                                # print(f"{sheet_name}: Linked Remarks (Row {current_row_number}) â†’ {target_ref}")
                        except Exception as e:
                            print(f"{sheet_name}: Failed to link Remarks at row {data_idx} â†’ {e}")

                # Soft-delete helper columns
                for helper_col in helper_cols.values():
                    if helper_col < len(data_row):
                        soft_delete_cell(data_row[helper_col])

                data_idx += 1

            row = data_idx + 1
            continue
        row += 1
    
    # Autofit columns
    for col in ws.columns:
        try:
            max_length = 0
            col_letter = get_column_letter(col[0].column)
            for cell in col:
                if cell.value is not None:
                    max_length = max(max_length, len(str(cell.value)) + 4) # for changing column width
            ws.column_dimensions[col_letter].width = max(1, max_length)
        except Exception:
            pass

    try:
        ws.column_dimensions["A"].width = 2
        ws.column_dimensions["B"].width = 6
    except Exception:
        pass

    if sheet_name.strip().lower() == "output - rule 4.1":
        print("Applying formatting to Output - Rule 4.1")

        # Adjust column widths
        col_widths = {
            "C": 10,
            "D": 25,  
            "E": 28,
            "F": 25,
            "G": 15,
            "H": 28,  
            "I": 15,
            "J": 28,
            "K": 15,
            "L": 28,
            "M": 28,
            "N": 28,
        }
        skip_rows = [2,3,4,6,8]
        for col_letter, width in col_widths.items():
            ws.column_dimensions[col_letter].width = width
        # Text wrapping 
        for row in ws.iter_rows():
            for cell in row:
                row_num = row[0].row        
                if row_num not in skip_rows:
                    cell.alignment = Alignment(wrap_text=True, vertical="top")

# Custom deletions
custom_deletions = {
    "Output - Rule 1": ["G","H"],
    "Output - Rule 2": ["I", "J"],
    "Output - Rule 3": ["I", "J"],
    #"Output - Rule 4": ["H", "I"],

}
for sheet_name, cols in custom_deletions.items():
    if sheet_name in wb.sheetnames:
        ws = wb[sheet_name]
        col_nums = sorted([column_index_from_string(c) for c in cols], reverse=True)
        for col_idx in col_nums:
            try:
                ws.delete_cols(col_idx)
                print(f"{sheet_name}: Deleted custom column {col_idx}")
            except Exception as e:
                print(f"{sheet_name}: Failed to delete column {col_idx}: {e}")

print("Applied successfully.")
wb.save(file_path)












































prompt = """

You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
 
Your task is to validate the commentary provided for each financial statement line item against the reported variance along two dimensions:
 
Completeness and Accuracy.
 
Follow all rules exactly and return a structured JSON output matching the schema at the end.
 
INPUT FORMAT
 
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
 
|<id>|<line item>|<difference>|<percentage change>|<variance_for_grouping>|<comment>|  
 
If variance_for_grouping is blank or missing, treat it as equal to difference.
 
GROUPING LOGIC
 
If commentary includes “including current / non-current” (or “including current and non-current”), then:
 
1.  Treat that commentary as group-level, applying to all related lines (e.g., “Loans and borrowings.”, “Loans and borrowings – current.”).
 
2.  Use the combined movement of those lines for completeness evaluation.
 
3.  Still return a separate JSON object for each line.
 
4.  Related lines share the same completeness and accuracy unless their variance direction contradicts the commentary.
 
COMPLETENESS — coverage of variance
 
A commentary passes Completeness if it explains ≥ 80 % of the absolute variance amount.
Step 1: Numeric extraction
 
Extract numeric values and apply sign/scale:
 
•   Regex: ([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
 
•   Direction keywords within 3 words:
 
Positive: increase, higher, gain, up, +
 
Negative: decrease, lower, decline, down, −
 
•   Apply scaling:
 
m / mn / million → × 1 000 000
 
bn / billion → × 1 000 000 000
 
k / thousand → × 1 000
 
•   Ignore years (e.g., 2025) or small identifiers (≤ 100 w/o units).
 
Step 2: Coverage calculation
 
Sum all valid numbers (with sign + scale).
 
coverage = explained variance ÷ absolute variance × 100 %
 
Step 3: Qualitative completeness

If not a single numeric data in entire commentary related to variance:
 
•   Accept qualitative explanations
•   Otherwise → Fail.

Completeness justification format
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)

Explains <coverage>% of <variance> variance. (<numeric breakdown>)
 
Examples:
 
•   Explains 100 % of 2.01 bn variance. (5.2 bn + 0.5 bn + 0.2 bn – 3.9 bn = 2.0 bn)
 
•   Explains 52.5 % of 162.9 m variance. (29.5 m + 26 m = 55.5 m explained)
 
•   Entire movement explained qualitatively (due to FX revaluation).
 
ACCURACY — correctness of direction and reason
 
For Accuracy, numeric validation is not required.
 
Evaluate only direction and reason.
 
Pass if:
 
1.  Commentary direction (increase/decrease) matches variance sign.
 
2.  Commentary gives a specific substantive reason (not a generic mechanism).
 
Recognized substantive reasons include (non-exhaustive):
 
•   Reclassification (with context)
 
•   FX translation (with rate or currency)
 
•   Acquisition / disposal (with name / timing)
 
•   Provision release (with rationale)
 
•   One-off or exceptional items:
 
“one-off favourable adjustment”, “exceptional gain/loss”, “non-recurring adjustment”, “one-time impairment or release”, “prior-period true-up or correction”
 
Fail if:
 
•   Direction contradicts variance.
 
•   Commentary is vague (“FX differences”, “reclassified”) without context.
 
•   Mixed increase / decrease with no dominant direction.
 
Accuracy justification format
 
Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).
 
Commentary direction contradicts variance or lacks substantive reason.
 
IMPLEMENTATION NOTES
 
•   Trim whitespace in parsed fields.
 
•   Convert extracted numbers to float after scaling.
 
•   Use sign of difference to determine direction.
 
•   Round monetary values to two decimals in the JSON.
 
OUTPUT SCHEMA
 
Return a JSON array of objects:
 
{
  "id": "string", 
  "line_item": "string",
  "completeness": "Pass" | "Fail",
  "completeness_justification": "string",
  "accuracy": "Pass" | "Fail",
  "accuracy_justification": "string",
  "Final_Status": "OK | Incomplete | Inaccurate | Inconclusive",
  "Remarks": "Commentary provided is valid | Commentary provided is inaccurate and incomplete | Commentary provided is incomplete | Commentary provided is inaccurate",
  "Overall_Justification": "<short summary combining the 2 checks>"

}


FINAL STATUS MAPPING LOGIC
If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is Inconclusive”
Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”


No additional text outside the JSON array.
 
EXAMPLE INPUT
 
|AI1_000001|Loans and borrowings.|-|265847823|0|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of ABC and DEF including 0.9 BN HQ XYZ. 0.2 BN FX & other movements."|
 
|AI1_000030|Loans and borrowings - current.|-|-2732813238|-30|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements."|
 
EXAMPLE OUTPUT
 
[
 
  {
    "id": "AI1_000001",  
    "line_item": "Loans and borrowings.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, loan drawdowns, FX movements).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  },
 
  {
    "id": "AI1_000030",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, drawdowns, FX adjustments).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  }
 
]

"""


input_table = """
|ID||Line Item|Account|Difference|% Change|Commentary|
ai034176|Accounts and other receivables - current.|-|-1.10328724775E9|-18.51942889573246|Decrease from Deferred financial costs of AED 57m for transaction fees utilized mainly from conversion of EBL loans to Equity.  There is also a decrease in KEPCO AED 10m due to returns of borrowed materials and other receivable & prepayments AED11m due to amortization.  These are partly offset by increase in interest receivable from FD amounting to AED 45m.

"""








[
  {
    "id": "ai034176",
    "line_item": "Accounts and other receivables - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 74.16% of 1,103.29m variance. (-57m -10m -11m +45m = -33m explained; however, the commentary provides qualitative reasons for the remaining movement, which is acceptable for completeness as per rules.)",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (deferred financial costs, returns of borrowed materials, amortization, and interest receivable).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": "Commentary provides both numeric and qualitative explanations for the decrease, matching the direction and giving substantive reasons."
  }
]

If no numeric data is found, pass completeness if commentary provides qualitative explanation.
If numeric data exists but covers < 80%, fail completeness.




prompt = """

You are an AI assistant reviewing Quarterly Financial Statement (FS) packs submitted by subsidiaries to the group company.
 
Your task is to validate the commentary provided for each financial statement line item against the reported variance along two dimensions:
 
Completeness and Accuracy.
 
Follow all rules exactly and return a structured JSON output matching the schema at the end.
 
INPUT FORMAT
 
Each record is provided one per line, pipe-delimited (leading/trailing pipes optional):
 
|<id>|<line item>|<difference>|<percentage change>|<variance_for_grouping>|<comment>|  
 
If variance_for_grouping is blank or missing, treat it as equal to difference.
 
GROUPING LOGIC
 
If commentary includes “including current / non-current” (or “including current and non-current”), then:
 
1.  Treat that commentary as group-level, applying to all related lines (e.g., “Loans and borrowings.”, “Loans and borrowings – current.”).
 
2.  Use the combined movement of those lines for completeness evaluation.
 
3.  Still return a separate JSON object for each line.
 
4.  Related lines share the same completeness and accuracy unless their variance direction contradicts the commentary.
 
COMPLETENESS — coverage of variance
 
A commentary passes Completeness if it explains ≥ 80 % of the absolute variance amount. If there is no explanation of variance amount and commentary provides a qualitative explanation, commentary passes completeness.
 
Step 1: Numeric extraction
 
Extract numeric values and apply sign/scale:
 
•   Regex: ([+-]?[0-9]*\.?[0-9]+)\s*(bn|billion|m|mn|million|k|thousand)?
 
•   Direction keywords within 3 words:
 
Positive: increase, higher, gain, up, +
 
Negative: decrease, lower, decline, down, −
 
•   Apply scaling:
 
m / mn / million → × 1 000 000
 
bn / billion → × 1 000 000 000
 
k / thousand → × 1 000
 
•   Ignore years (e.g., 2025) or small identifiers (≤ 100 w/o units).
 
Step 2: Coverage calculation
 
Sum all valid numbers (with sign + scale).
 
coverage = explained variance ÷ absolute variance × 100 %
 
Step 3: Qualitative completeness
 
If no numeric data related to coverage in entire commentary:
 
•   Accept qualitative explanations even if there is no quantification.
 
•   Otherwise → Fail.
 
Completeness justification format
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)
 
Explains <coverage>% of <variance> variance. (<numeric breakdown>)

Explains 100% of <variance> variance.
 
Examples:
 
•   Explains 100 % of 2.01 bn variance. (5.2 bn + 0.5 bn + 0.2 bn – 3.9 bn = 2.0 bn)
 
•   Explains 52.5 % of 162.9 m variance. (29.5 m + 26 m = 55.5 m explained)
 
•   Entire movement explained qualitatively (Since there is no numeric breakdown provided in the commentary).
 
ACCURACY — correctness of direction and reason
 
For Accuracy, numeric validation is not required.
 
Evaluate only direction and reason.
 
Pass if:
 
1.  Commentary direction (increase/decrease) matches variance sign.
 
2.  Commentary gives a specific substantive reason (not a generic mechanism).
 
Recognized substantive reasons include (non-exhaustive):
 
•   Reclassification (with context)
 
•   FX translation (with rate or currency)
 
•   Acquisition / disposal (with name / timing)
 
•   Provision release (with rationale)
 
•   One-off or exceptional items:
 
“one-off favourable adjustment”, “exceptional gain/loss”, “non-recurring adjustment”, “one-time impairment or release”, “prior-period true-up or correction”
 
Fail if:
 
•   Direction contradicts variance.
 
•   Commentary is vague (“FX differences”, “reclassified”) without context.
 
•   Mixed increase / decrease with no dominant direction.
 
Accuracy justification format
 
Commentary direction matches variance and provides substantive reason (e.g., FX impact, impairment, one-off adjustment).
 
Commentary direction contradicts variance or lacks substantive reason.
 
IMPLEMENTATION NOTES
 
•   Trim whitespace in parsed fields.
 
•   Convert extracted numbers to float after scaling.
 
•   Use sign of difference to determine direction.
 
•   Round monetary values to two decimals in the JSON.
 
OUTPUT SCHEMA
 
Return a JSON array of objects:
 
{
  "id": "string", 
  "line_item": "string",
  "completeness": "Pass" | "Fail",
  "completeness_justification": "string",
  "accuracy": "Pass" | "Fail",
  "accuracy_justification": "string",
  "Final_Status": "OK | Incomplete | Inaccurate | Inconclusive",
  "Remarks": "Commentary provided is valid | Commentary provided is inaccurate and incomplete | Commentary provided is incomplete | Commentary provided is inaccurate",
  "Overall_Justification": "<short summary combining the 2 checks>"

}


FINAL STATUS MAPPING LOGIC
If Existence = Fail → Final_Status = "Missing" and Remarks = “Commentary is not provided”
Else If Completeness = Fail and Accuracy = Fail → Final_Status = Incomplete and inaccurate" and Remarks = “Commentary provided is incomplete and inaccurate”
Else if Completeness = Fail → Final_Status = "Incomplete" and Remarks = “Commentary provided is incomplete”
Else if Accuracy = Fail → Final_Status = "Inaccurate" and Remarks = “Commentary provided is inaccurate”
Else if Accuracy = Inconclusive → Final_Status = "Inconclusive" and Remarks = “Commentary provided is inconclusive”
Else → Final_Status = "OK" and Remarks = “Commentary provided is valid”


No additional text outside the JSON array.
 
EXAMPLE INPUT
 
|AI1_000001|Loans and borrowings.|265313810|0|265847823|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of ABC and DEF including 0.9 BN HQ XYZ. 0.2 BN FX & other movements."|
 
|AI1_000030|Loans and borrowings - current.|2732813238|-30|-2732813238|"Loans and borrowing (Including current/ non current) decreased by 2.5 BN due to: 2.8 BN Bond repayment & 1.4 BN standard debt repayments. 1.5 BN loan drawdowns due to construction of M2RO and S4RO including 0.9 BN HQ RCF. 0.2 BN FX & other movements."|

|AI1_000031|Loans and borrowings - current.|2732813238|-30|-2732813238|"Due to Bond repayment."|

EXAMPLE OUTPUT
 
[
 
  {
    "id": "AI1_000001",  
    "line_item": "Loans and borrowings.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, loan drawdowns, FX movements).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  },
  {
    "id": "AI1_000030",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance. (-2.8bn –1.4bn +1.5bn +0.9bn +0.2bn = -1.6bn → rounded to total group movement of -2.47bn, acceptable within threshold).",
    "accuracy": "Pass",
    "accuracy_justification": "Commentary direction (decrease) matches variance and provides substantive reasons (bond repayment, debt repayment, drawdowns, FX adjustments).",
    "Final_Status": "OK",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance and matches the direction of movement"

  },
  {
    "id": "AI1_000031",  
    "line_item": "Loans and borrowings - current.",
    "completeness": "Pass",
    "completeness_justification": "Explains 100% of 2.47bn combined variance as there is no percentage mentioned in commentary.",
    "accuracy": "Fail",
    "accuracy_justification": "There is no direction provided in the commentary that states increase or decrease",
    "Final_Status": "Fail",
    "Remarks": "Commentary provided is valid",
    "Overall_Justification": Commentary provided explains the variance, but does not mention the direction of movement"

  }



]

"""








Assets,Total Non-current Assets,Total Current assets,Total Assets.,Equity and liabilities,Total Equity.,Liabilities,Total Non-current Liabilities,Total Current liabilities,Total Liabilities.,Total equity and liabilities.,SoFP Validation_Commentary,Operating Profit.,Profit before income tax.,Continuing Operations Income.,Profit for the year.,Profit/(loss) attributable to:,SoPL Validation_Commentary,Total Opening Balance,Movement during the period:,Closing Balance,Total Movement,Total Movement Changes,Validation,Total Movements,Total Movement ,Opening Balance,Total comprehensive income of the year,Items that will not be reclassified to profit or loss,Items that are or may be reclassified subsequently to profit or loss,Transactions with the Owner ,Closing balance as per Note,Validation - comparative period,Non-current ,GN 28 - Payable to project companies,GN 28 - Other Non-Current Liabilities,At period end,Current,GN 28 - Accounts payable due to third parties.,GN 28 - Accrued expenses,Related parties payables - current,GN 28 - Accounts payable due to related parties.,GN 28 - Retention payables.,Current Tax liabilities,As per SoFP,Non-current & Current total














with pd.ExcelWriter(
    file_path,
    engine="openpyxl",
    mode="a",                    # append mode (don’t overwrite)
    if_sheet_exists="overlay"    # allows writing into existing sheet
) as writer:
    df.to_excel(
        writer,
        sheet_name=sheet_name,
        startrow=startrow,       # choose where to start
        startcol=0,              # or any specific column
        index=False,
        header=False
    )






# Create a writer and attach the workbook
writer = pd.ExcelWriter(file_path, engine='openpyxl')
writer.book = workbook
writer.sheets = {ws.title: ws for ws in workbook.worksheets}

# Example: write DataFrame starting at row 10, column 3 (zero-indexed → Excel row 11, column D)
df.to_excel(
    writer,
    sheet_name=sheet_name,
    startrow=10,
    startcol=3,
    index=False,
    header=False
)

# Save changes
writer.close()




with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
    writer.book = book
    writer.sheets = {ws.title: ws for ws in book.worksheets}

    # Write DataFrame to a specific sheet and start position
    # startrow and startcol are zero-indexed
    df.to_excel(
        writer,
        sheet_name="Sheet1",   # Existing sheet name
        startrow=5,            # Write starting at row 6 (since 0-indexed)
        startcol=2,            # Write starting at column C
        index=False,
        header=False           # Set to True if you want to include headers
    )







import fsspec
from pathlib import Path

# Connect to OneLake (Azure Data Lake / OneLake)
fs = fsspec.filesystem(
    "abfs",  # Azure Blob / ADLS filesystem
    account_name="your_account_name",
    account_key="your_account_key"  # or use SAS token / credentials
)

oneLake_folder = "your-container-name/path/to/folder"

# Recursively find all CSV files
csv_files = [
    file for file in fs.find(oneLake_folder) if file.endswith(".csv")
]

# Print filenames only (without full path)
for file_path in csv_files:
    print(Path(file_path).name)




# Python standard libraries
import os, json, csv, glob, re, hashlib
from math import ceil
from pathlib import Path
from typing import List
from functools import reduce

# PySpark
from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, concat, trim, regexp_extract, when, abs, isnan, lower
from pyspark.sql.types import StringType, DoubleType

# Pandas / Excel (do not reinstall)
import pandas as pd
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows

# OpenAI (make sure version is >=1.0.0)
from openai import OpenAI  # or AzureOpenAI if using Azure





import pandas as pd
from pyspark.sql import SparkSession
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, concat, trim, regexp_extract, when, abs
from pyspark.sql.types import StringType, DoubleType
import os, json, fsspec, csv, glob
from math import ceil
from openai import AzureOpenAI
import re
import hashlib
from pathlib import Path
from typing import List
from openai import OpenAI
from pyspark.sql.functions import isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
from pyspark.sql.functions import trim, lower






final_df = final_df.withColumn(
        "Existence_Justification",
        F.when(
            (col("PassFailFlag") == "Fail") & (col("Existence_Status") == "Fail"),
            F.lit("Commentary was not provided")
        ).otherwise(col("Existence_Justification"))
    )


    header_row = 1
    final_header_col = None
    for col in range(1, col_ref_sheet.max_column + 1):
        header_val = col_ref_sheet.cell(row=header_row, column=col).value
        if header_val and str(header_val).strip().lower() == "final_column_header":
            final_header_col = col
            break
 
    # If found, clean each value under it
    if final_header_col:
        for row in range(2, col_ref_sheet.max_row + 1):
            val = col_ref_sheet.cell(row=row, column=final_header_col).value
            if val:
                val = str(val)
                val = re.sub(r"\(", "_", val)  
                val = re.sub(r"\)", "", val)  
                val = re.sub(r"_+", "_", val)  
                val = val.strip("_ ")
                col_ref_sheet.cell(row=row, column=final_header_col, value=val)







from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
import re
 
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    try:
        return float(val)
    except:
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status
 
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            return None
    except:
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary", "row_no"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    row_no_val = df.filter(F.lower(F.col("fsli")) == "commentary").select("row_no").first()["row_no"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val, row_no_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id", "row_no"
    ])
 
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        # "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname = row["sheetname"]
        account = row["account"]
        diff_col = row["differenceheader"]
        pc_col = row["changeheader"]
        comm_col = row["commentaryheader"]
        mappingtableid = row["id"]
 
        path = sheet_paths.get(sheetname)
        if not path:
            continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except:
                cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]):
            continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn(
            "PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id"),
            col("row_no").alias("row_no")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
 
combined_df = reduce(
    DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]
) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window))
 
    try:
        existing_df = spark.read.format("delta").load(final_output_path)
        max_id_row = (
            existing_df.select("id")
            .rdd.map(lambda r: int(re.sub(r"\D", "", r["id"])) if r["id"] else 0)
            .max()
        )
        start_id = max_id_row + 1
        print(f"existing max ID: {max_id_row}")
    except Exception:
        start_id = 1
        print("No existing table found, starting from ID 1.")
 
    final_df = final_df.withColumn(
        "id",
        F.concat(F.lit("AI1_"), F.format_string("%06d", (col("row_num") + F.lit(start_id - 1))))
    ).drop("row_num")
 
    cols = ["id", "row_no"] + [c for c in final_df.columns if c not in ["id", "row_no"]]
    final_df.select(*cols) \
        .write.format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(final_output_path)
 
    print("Final output written successfully ")
else:
    print("No valid data found to write.")




























# final all sheet + gn_18
 
 
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
 
# Config
base_path =  "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
 
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
# Utility functions
def safe_float(val):
    try: return float(val)
    except: return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0: return None
    except: return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
        path = sheet_paths.get(sheetname)
        if not path: continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except: cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]): continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn("PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                          .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                          .drop("row_num")
 
    cols = ["id"] + [c for c in final_df.columns if c != "id"]
    final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
    print(" Final output written successfully .")
else:
    print(" No valid data found to write.")
 


















# final all sheet + gn_18 [with try and logs]
 
 
 
import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Rule4Validation")
 
# Initialize Spark session
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
# Config
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    if val is None or val == "":
        return 0.0
    try:
        return float(val)
    except Exception as e:
        logger.warning(f"Unexpected value for float conversion: {val} — {e}")
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    try:
        if diff_val > 10_000_000:
            flag = "Fail"
        elif pc_val > 10 and diff_val >= 5_000_000:
            flag = "Fail"
        else:
            flag = "Pass"
        existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
        return flag, existence_status
    except Exception as e:
        logger.error(f"Validation error: {e}")
        return "Error", "Error"
 
# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            logger.warning("No GN1811 data found for txnID.")
            return None
    except Exception as e:
        logger.error(f"Error loading GN1811 sheet: {e}")
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        try:
            row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
            if row_df.count() > 0:
                rows[label] = row_df.collect()[0].asDict()
        except Exception as e:
            logger.warning(f"Error extracting {label} row: {e}")
 
    try:
        id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    except Exception as e:
        logger.error(f"Error extracting id from GN1811: {e}")
        id_val = "unknown"
 
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
    result = []
 
    for col_name in asset_cols:
        try:
            diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
            pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
            comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
            flag, existence_status = validate_row(diff_val, pc_val, comm_val)
            result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
        except Exception as e:
            logger.error(f"Error processing column {col_name}: {e}")
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    try:
        mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    except Exception as e:
        logger.error(f"Error loading mapping sheet: {e}")
        return None
 
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        try:
            sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
            path = sheet_paths.get(sheetname)
            if not path:
                logger.warning(f"No path found for sheet: {sheetname}")
                continue
 
            if sheetname not in cached_sheets:
                try:
                    df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                    cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
                except Exception as e:
                    logger.error(f"Error loading sheet {sheetname}: {e}")
                    cached_sheets[sheetname] = None
            df = cached_sheets.get(sheetname)
            if df is None:
                logger.warning(f"Sheet {sheetname}  has no data Corresponding to the given txnID .")
                continue
 
            missing_cols = [c for c in [diff_col, pc_col, comm_col] if c not in df.columns]
            if missing_cols:
                logger.warning(f"Sheet {sheetname} is missing required columns: {missing_cols}")
                continue
 
            if df.filter(col("txnID") == txn_id).count() == 0:
                logger.info(f"Sheet {sheetname} has no data for txnID: {txn_id}")
                continue
 
            df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
            fail_1 = F.abs(col(diff_col)) > 10_000_000
            fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
            df_casted = df_casted.withColumn("PassFailFlag",
                F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
                 .when(fail_1 | fail_2, "Fail")
                 .otherwise("Pass")
            )
 
            formatted = df_casted.select(
                F.lit(sheetname).alias("sheetname"),
                F.lit(mappingtableid).alias("mappingtableid"),
                col("fsli"),
                F.lit(account).alias("account"),
                col(diff_col).alias("difference"),
                col(pc_col).alias("pc_change"),
                col(comm_col).alias("commentary"),
                col("PassFailFlag"),
                F.when(
                    (col("PassFailFlag") == "Fail") &
                    (col(comm_col).isNotNull()) &
                    (~isnan(col(comm_col))) &
                    (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                    "Pass"
                ).otherwise("Fail").alias("Existence_Status"),
                F.lit("").alias("Existence_Justification"),
                F.lit(txn_id).alias("txnID"),
                col("id").alias("odsid"),
                col("id").alias("id")
            )
 
            final_dfs.append(formatted)
        except Exception as e:
            logger.error(f"Error processing mapping row: {e}")
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
try:
    gn1811_df = process_gn1811()
    mapping_df = process_mapping_sheets()
    combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
    if combined_df:
        window = Window.orderBy(F.monotonically_increasing_id())
        final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                              .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                              .drop("row_num")
 
        cols = ["id"] + [c for c in final_df.columns if c != "id"]
        final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
        logger.info(" Final output written successfully.")
    else:
        logger.warning(" No valid data found to write.")
except Exception as e:
    logger.critical(f"Unhandled error during final write: {e}")
 
























F.when(
    (F.col("PassFailFlag") == "Fail") &
    (F.col(comm_col).isNotNull()) &
    (~isnan(F.col(comm_col))) &
    (~F.trim(F.col(comm_col)).isin("", "0", "0.0")),
    "Pass"
).otherwise("Fail").alias("Existence_Status"),

F.lit("").alias("Existence_Justification")


�� ADDH Intercompany dividend payable

joined_df = joined_df.withColumn(
    "fsli",
    F.regexp_replace(F.regexp_replace(F.regexp_replace("fsli", r"[^\x00-\x7F]", ""), r"\\s+", " "), r"^\\s+|\\s+$", "")
)


from openpyxl import load_workbook
from openpyxl.workbook.calc_props import CalcProperties

def enable_auto_calc(file_in, file_out):
    wb = load_workbook(file_in)

    # Check if _calcPr exists, if not, create one
    if not hasattr(wb, '_calcPr') or wb._calcPr is None:
        wb._calcPr = CalcProperties(calcMode='auto', fullCalcOnLoad=True)
    else:
        wb._calcPr.calcMode = 'auto'
        wb._calcPr.fullCalcOnLoad = True

    wb.save(file_out)
    print(f"Saved workbook with automatic calculation: {file_out}")

# Example usage
enable_auto_calc('input.xlsx', 'output_auto_calc.xlsx')




import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Find workbookPr element and update calcMode attribute
    # Excel namespace
    ns = {'ns': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}
    workbookPr = root.find('ns:workbookPr', ns)
    
    if workbookPr is None:
        # If not found, create it
        workbookPr = etree.Element('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}workbookPr')
        root.insert(0, workbookPr)
    
    # Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 5. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 6. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                # archive name should be relative to temp_dir
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 7. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage example
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Extract the namespace from root tag, e.g. '{namespace}workbook'
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    ns = {'ns': ns_uri}
    
    # 5. Find workbookPr element with the correct namespace
    workbookPr = root.find('ns:workbookPr', namespaces=ns)
    
    if workbookPr is None:
        # Create workbookPr if it doesn't exist
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)
    
    # 6. Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 7. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 8. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 9. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')





import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # Clean temp dir if it exists
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract the .xlsx contents
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Path to workbook.xml
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')

    # Step 3: Parse workbook.xml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()

    # Step 4: Extract the namespace dynamically
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 5: Find or create <workbookPr>
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 6: Set or update calcMode to "auto"
    workbookPr.set('calcMode', 'auto')

    # Optional: Remove manual-related flags that force manual behavior
    for attr in ['fullCalcOnLoad', 'forceFullCalc']:
        if attr in workbookPr.attrib:
            del workbookPr.attrib[attr]

    # Step 7: Save the modified workbook.xml
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 8: Repackage everything into a new .xlsx file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)

    # Step 9: Cleanup
    shutil.rmtree(temp_dir)

# ✅ Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'

    # Clean up and create temp dir
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Modify workbook.xml
    workbook_xml = os.path.join(temp_dir, 'xl', 'workbook.xml')
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml, parser)
    root = tree.getroot()

    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 3: Find/create workbookPr
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 4: Set calcMode and cleanup other attributes
    workbookPr.attrib.clear()  # clear all attributes
    workbookPr.set('calcMode', 'auto')

    # Optional: Add Excel default calcId (can help with Excel Online)
    workbookPr.set('calcId', '122211')  # example value from new Excel files

    # Step 5: Write back
    tree.write(workbook_xml, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 6: Re-zip to output_path
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for root_dir, dirs, files in os.walk(temp_dir):
            for file in files:
                abs_path = os.path.join(root_dir, file)
                rel_path = os.path.relpath(abs_path, temp_dir)
                zip_out.write(abs_path, rel_path)

    # Step 7: Cleanup
    shutil.rmtree(temp_dir)

# Example usage (paths must point to accessible Lakehouse or OneLake locations)
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')
