from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
import re
 
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    try:
        return float(val)
    except:
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status
 
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            return None
    except:
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary", "row_no"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    row_no_val = df.filter(F.lower(F.col("fsli")) == "commentary").select("row_no").first()["row_no"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val, row_no_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id", "row_no"
    ])
 
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        # "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname = row["sheetname"]
        account = row["account"]
        diff_col = row["differenceheader"]
        pc_col = row["changeheader"]
        comm_col = row["commentaryheader"]
        mappingtableid = row["id"]
 
        path = sheet_paths.get(sheetname)
        if not path:
            continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except:
                cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]):
            continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn(
            "PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id"),
            col("row_no").alias("row_no")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
 
combined_df = reduce(
    DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]
) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window))
 
    try:
        existing_df = spark.read.format("delta").load(final_output_path)
        max_id_row = (
            existing_df.select("id")
            .rdd.map(lambda r: int(re.sub(r"\D", "", r["id"])) if r["id"] else 0)
            .max()
        )
        start_id = max_id_row + 1
        print(f"existing max ID: {max_id_row}")
    except Exception:
        start_id = 1
        print("No existing table found, starting from ID 1.")
 
    final_df = final_df.withColumn(
        "id",
        F.concat(F.lit("AI1_"), F.format_string("%06d", (col("row_num") + F.lit(start_id - 1))))
    ).drop("row_num")
 
    cols = ["id", "row_no"] + [c for c in final_df.columns if c not in ["id", "row_no"]]
    final_df.select(*cols) \
        .write.format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(final_output_path)
 
    print("Final output written successfully ")
else:
    print("No valid data found to write.")




























# final all sheet + gn_18
 
 
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
 
# Config
base_path =  "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
 
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
# Utility functions
def safe_float(val):
    try: return float(val)
    except: return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0: return None
    except: return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
        path = sheet_paths.get(sheetname)
        if not path: continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except: cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]): continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn("PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                          .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                          .drop("row_num")
 
    cols = ["id"] + [c for c in final_df.columns if c != "id"]
    final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
    print(" Final output written successfully .")
else:
    print(" No valid data found to write.")
 


















# final all sheet + gn_18 [with try and logs]
 
 
 
import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Rule4Validation")
 
# Initialize Spark session
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
# Config
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    if val is None or val == "":
        return 0.0
    try:
        return float(val)
    except Exception as e:
        logger.warning(f"Unexpected value for float conversion: {val} — {e}")
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    try:
        if diff_val > 10_000_000:
            flag = "Fail"
        elif pc_val > 10 and diff_val >= 5_000_000:
            flag = "Fail"
        else:
            flag = "Pass"
        existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
        return flag, existence_status
    except Exception as e:
        logger.error(f"Validation error: {e}")
        return "Error", "Error"
 
# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            logger.warning("No GN1811 data found for txnID.")
            return None
    except Exception as e:
        logger.error(f"Error loading GN1811 sheet: {e}")
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        try:
            row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
            if row_df.count() > 0:
                rows[label] = row_df.collect()[0].asDict()
        except Exception as e:
            logger.warning(f"Error extracting {label} row: {e}")
 
    try:
        id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    except Exception as e:
        logger.error(f"Error extracting id from GN1811: {e}")
        id_val = "unknown"
 
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
    result = []
 
    for col_name in asset_cols:
        try:
            diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
            pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
            comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
            flag, existence_status = validate_row(diff_val, pc_val, comm_val)
            result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
        except Exception as e:
            logger.error(f"Error processing column {col_name}: {e}")
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    try:
        mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    except Exception as e:
        logger.error(f"Error loading mapping sheet: {e}")
        return None
 
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        try:
            sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
            path = sheet_paths.get(sheetname)
            if not path:
                logger.warning(f"No path found for sheet: {sheetname}")
                continue
 
            if sheetname not in cached_sheets:
                try:
                    df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                    cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
                except Exception as e:
                    logger.error(f"Error loading sheet {sheetname}: {e}")
                    cached_sheets[sheetname] = None
            df = cached_sheets.get(sheetname)
            if df is None:
                logger.warning(f"Sheet {sheetname}  has no data Corresponding to the given txnID .")
                continue
 
            missing_cols = [c for c in [diff_col, pc_col, comm_col] if c not in df.columns]
            if missing_cols:
                logger.warning(f"Sheet {sheetname} is missing required columns: {missing_cols}")
                continue
 
            if df.filter(col("txnID") == txn_id).count() == 0:
                logger.info(f"Sheet {sheetname} has no data for txnID: {txn_id}")
                continue
 
            df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
            fail_1 = F.abs(col(diff_col)) > 10_000_000
            fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
            df_casted = df_casted.withColumn("PassFailFlag",
                F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
                 .when(fail_1 | fail_2, "Fail")
                 .otherwise("Pass")
            )
 
            formatted = df_casted.select(
                F.lit(sheetname).alias("sheetname"),
                F.lit(mappingtableid).alias("mappingtableid"),
                col("fsli"),
                F.lit(account).alias("account"),
                col(diff_col).alias("difference"),
                col(pc_col).alias("pc_change"),
                col(comm_col).alias("commentary"),
                col("PassFailFlag"),
                F.when(
                    (col("PassFailFlag") == "Fail") &
                    (col(comm_col).isNotNull()) &
                    (~isnan(col(comm_col))) &
                    (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                    "Pass"
                ).otherwise("Fail").alias("Existence_Status"),
                F.lit("").alias("Existence_Justification"),
                F.lit(txn_id).alias("txnID"),
                col("id").alias("odsid"),
                col("id").alias("id")
            )
 
            final_dfs.append(formatted)
        except Exception as e:
            logger.error(f"Error processing mapping row: {e}")
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
try:
    gn1811_df = process_gn1811()
    mapping_df = process_mapping_sheets()
    combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
    if combined_df:
        window = Window.orderBy(F.monotonically_increasing_id())
        final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                              .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                              .drop("row_num")
 
        cols = ["id"] + [c for c in final_df.columns if c != "id"]
        final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
        logger.info(" Final output written successfully.")
    else:
        logger.warning(" No valid data found to write.")
except Exception as e:
    logger.critical(f"Unhandled error during final write: {e}")
 
























F.when(
    (F.col("PassFailFlag") == "Fail") &
    (F.col(comm_col).isNotNull()) &
    (~isnan(F.col(comm_col))) &
    (~F.trim(F.col(comm_col)).isin("", "0", "0.0")),
    "Pass"
).otherwise("Fail").alias("Existence_Status"),

F.lit("").alias("Existence_Justification")


�� ADDH Intercompany dividend payable

joined_df = joined_df.withColumn(
    "fsli",
    F.regexp_replace(F.regexp_replace(F.regexp_replace("fsli", r"[^\x00-\x7F]", ""), r"\\s+", " "), r"^\\s+|\\s+$", "")
)


from openpyxl import load_workbook
from openpyxl.workbook.calc_props import CalcProperties

def enable_auto_calc(file_in, file_out):
    wb = load_workbook(file_in)

    # Check if _calcPr exists, if not, create one
    if not hasattr(wb, '_calcPr') or wb._calcPr is None:
        wb._calcPr = CalcProperties(calcMode='auto', fullCalcOnLoad=True)
    else:
        wb._calcPr.calcMode = 'auto'
        wb._calcPr.fullCalcOnLoad = True

    wb.save(file_out)
    print(f"Saved workbook with automatic calculation: {file_out}")

# Example usage
enable_auto_calc('input.xlsx', 'output_auto_calc.xlsx')




import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Find workbookPr element and update calcMode attribute
    # Excel namespace
    ns = {'ns': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}
    workbookPr = root.find('ns:workbookPr', ns)
    
    if workbookPr is None:
        # If not found, create it
        workbookPr = etree.Element('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}workbookPr')
        root.insert(0, workbookPr)
    
    # Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 5. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 6. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                # archive name should be relative to temp_dir
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 7. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage example
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Extract the namespace from root tag, e.g. '{namespace}workbook'
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    ns = {'ns': ns_uri}
    
    # 5. Find workbookPr element with the correct namespace
    workbookPr = root.find('ns:workbookPr', namespaces=ns)
    
    if workbookPr is None:
        # Create workbookPr if it doesn't exist
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)
    
    # 6. Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 7. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 8. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 9. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')





import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # Clean temp dir if it exists
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract the .xlsx contents
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Path to workbook.xml
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')

    # Step 3: Parse workbook.xml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()

    # Step 4: Extract the namespace dynamically
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 5: Find or create <workbookPr>
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 6: Set or update calcMode to "auto"
    workbookPr.set('calcMode', 'auto')

    # Optional: Remove manual-related flags that force manual behavior
    for attr in ['fullCalcOnLoad', 'forceFullCalc']:
        if attr in workbookPr.attrib:
            del workbookPr.attrib[attr]

    # Step 7: Save the modified workbook.xml
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 8: Repackage everything into a new .xlsx file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)

    # Step 9: Cleanup
    shutil.rmtree(temp_dir)

# ✅ Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'

    # Clean up and create temp dir
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Modify workbook.xml
    workbook_xml = os.path.join(temp_dir, 'xl', 'workbook.xml')
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml, parser)
    root = tree.getroot()

    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 3: Find/create workbookPr
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 4: Set calcMode and cleanup other attributes
    workbookPr.attrib.clear()  # clear all attributes
    workbookPr.set('calcMode', 'auto')

    # Optional: Add Excel default calcId (can help with Excel Online)
    workbookPr.set('calcId', '122211')  # example value from new Excel files

    # Step 5: Write back
    tree.write(workbook_xml, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 6: Re-zip to output_path
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for root_dir, dirs, files in os.walk(temp_dir):
            for file in files:
                abs_path = os.path.join(root_dir, file)
                rel_path = os.path.relpath(abs_path, temp_dir)
                zip_out.write(abs_path, rel_path)

    # Step 7: Cleanup
    shutil.rmtree(temp_dir)

# Example usage (paths must point to accessible Lakehouse or OneLake locations)
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')
