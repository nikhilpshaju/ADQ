with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
    writer.book = book
    writer.sheets = {ws.title: ws for ws in book.worksheets}

    # Write DataFrame to a specific sheet and start position
    # startrow and startcol are zero-indexed
    df.to_excel(
        writer,
        sheet_name="Sheet1",   # Existing sheet name
        startrow=5,            # Write starting at row 6 (since 0-indexed)
        startcol=2,            # Write starting at column C
        index=False,
        header=False           # Set to True if you want to include headers
    )







import fsspec
from pathlib import Path

# Connect to OneLake (Azure Data Lake / OneLake)
fs = fsspec.filesystem(
    "abfs",  # Azure Blob / ADLS filesystem
    account_name="your_account_name",
    account_key="your_account_key"  # or use SAS token / credentials
)

oneLake_folder = "your-container-name/path/to/folder"

# Recursively find all CSV files
csv_files = [
    file for file in fs.find(oneLake_folder) if file.endswith(".csv")
]

# Print filenames only (without full path)
for file_path in csv_files:
    print(Path(file_path).name)




# Python standard libraries
import os, json, csv, glob, re, hashlib
from math import ceil
from pathlib import Path
from typing import List
from functools import reduce

# PySpark
from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, concat, trim, regexp_extract, when, abs, isnan, lower
from pyspark.sql.types import StringType, DoubleType

# Pandas / Excel (do not reinstall)
import pandas as pd
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows

# OpenAI (make sure version is >=1.0.0)
from openai import OpenAI  # or AzureOpenAI if using Azure





import pandas as pd
from pyspark.sql import SparkSession
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, concat, trim, regexp_extract, when, abs
from pyspark.sql.types import StringType, DoubleType
import os, json, fsspec, csv, glob
from math import ceil
from openai import AzureOpenAI
import re
import hashlib
from pathlib import Path
from typing import List
from openai import OpenAI
from pyspark.sql.functions import isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
from pyspark.sql.functions import trim, lower






final_df = final_df.withColumn(
        "Existence_Justification",
        F.when(
            (col("PassFailFlag") == "Fail") & (col("Existence_Status") == "Fail"),
            F.lit("Commentary was not provided")
        ).otherwise(col("Existence_Justification"))
    )


    header_row = 1
    final_header_col = None
    for col in range(1, col_ref_sheet.max_column + 1):
        header_val = col_ref_sheet.cell(row=header_row, column=col).value
        if header_val and str(header_val).strip().lower() == "final_column_header":
            final_header_col = col
            break
 
    # If found, clean each value under it
    if final_header_col:
        for row in range(2, col_ref_sheet.max_row + 1):
            val = col_ref_sheet.cell(row=row, column=final_header_col).value
            if val:
                val = str(val)
                val = re.sub(r"\(", "_", val)  
                val = re.sub(r"\)", "", val)  
                val = re.sub(r"_+", "_", val)  
                val = val.strip("_ ")
                col_ref_sheet.cell(row=row, column=final_header_col, value=val)







from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
import re
 
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    try:
        return float(val)
    except:
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status
 
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            return None
    except:
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary", "row_no"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    row_no_val = df.filter(F.lower(F.col("fsli")) == "commentary").select("row_no").first()["row_no"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val, row_no_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id", "row_no"
    ])
 
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        # "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname = row["sheetname"]
        account = row["account"]
        diff_col = row["differenceheader"]
        pc_col = row["changeheader"]
        comm_col = row["commentaryheader"]
        mappingtableid = row["id"]
 
        path = sheet_paths.get(sheetname)
        if not path:
            continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except:
                cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]):
            continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn(
            "PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id"),
            col("row_no").alias("row_no")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
 
combined_df = reduce(
    DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]
) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window))
 
    try:
        existing_df = spark.read.format("delta").load(final_output_path)
        max_id_row = (
            existing_df.select("id")
            .rdd.map(lambda r: int(re.sub(r"\D", "", r["id"])) if r["id"] else 0)
            .max()
        )
        start_id = max_id_row + 1
        print(f"existing max ID: {max_id_row}")
    except Exception:
        start_id = 1
        print("No existing table found, starting from ID 1.")
 
    final_df = final_df.withColumn(
        "id",
        F.concat(F.lit("AI1_"), F.format_string("%06d", (col("row_num") + F.lit(start_id - 1))))
    ).drop("row_num")
 
    cols = ["id", "row_no"] + [c for c in final_df.columns if c not in ["id", "row_no"]]
    final_df.select(*cols) \
        .write.format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(final_output_path)
 
    print("Final output written successfully ")
else:
    print("No valid data found to write.")




























# final all sheet + gn_18
 
 
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
 
# Config
base_path =  "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
 
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
# Utility functions
def safe_float(val):
    try: return float(val)
    except: return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    if diff_val > 10_000_000:
        flag = "Fail"
    elif pc_val > 10 and diff_val >= 5_000_000:
        flag = "Fail"
    else:
        flag = "Pass"
    existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
    return flag, existence_status# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0: return None
    except: return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
        if row_df.count() > 0:
            rows[label] = row_df.collect()[0].asDict()
 
    id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
 
    result = []
    for col_name in asset_cols:
        diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
        pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
        comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
        flag, existence_status = validate_row(diff_val, pc_val, comm_val)
        result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
        path = sheet_paths.get(sheetname)
        if not path: continue
 
        if sheetname not in cached_sheets:
            try:
                df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
            except: cached_sheets[sheetname] = None
 
        df = cached_sheets.get(sheetname)
        if not df or any(c not in df.columns for c in [diff_col, pc_col, comm_col]): continue
 
        df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
        fail_1 = F.abs(col(diff_col)) > 10_000_000
        fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
        df_casted = df_casted.withColumn("PassFailFlag",
            F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
             .when(fail_1 | fail_2, "Fail")
             .otherwise("Pass")
        )
 
        formatted = df_casted.select(
            F.lit(sheetname).alias("sheetname"),
            F.lit(mappingtableid).alias("mappingtableid"),
            col("fsli"),
            F.lit(account).alias("account"),
            col(diff_col).alias("difference"),
            col(pc_col).alias("pc_change"),
            col(comm_col).alias("commentary"),
            col("PassFailFlag"),
            F.when(
                (col("PassFailFlag") == "Fail") &
                (col(comm_col).isNotNull()) &
                (~isnan(col(comm_col))) &
                (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                "Pass"
            ).otherwise("Fail").alias("Existence_Status"),
            F.lit("").alias("Existence_Justification"),
            F.lit(txn_id).alias("txnID"),
            col("id").alias("odsid"),
            col("id").alias("id")
        )
 
        final_dfs.append(formatted)
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
gn1811_df = process_gn1811()
mapping_df = process_mapping_sheets()
combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
if combined_df:
    window = Window.orderBy(F.monotonically_increasing_id())
    final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                          .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                          .drop("row_num")
 
    cols = ["id"] + [c for c in final_df.columns if c != "id"]
    final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
    print(" Final output written successfully .")
else:
    print(" No valid data found to write.")
 


















# final all sheet + gn_18 [with try and logs]
 
 
 
import logging
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import col, isnan
from pyspark.sql import Window
from functools import reduce
from pyspark.sql import DataFrame
 
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Rule4Validation")
 
# Initialize Spark session
spark = SparkSession.builder \
    .appName("IntegratedRule4Validation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
 
spark.catalog.clearCache()
 
# Config
base_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables"
txn_id = "MS1_1000001"
final_output_path = f"{base_path}/referencerule4_combined"
 
def safe_float(val):
    if val is None or val == "":
        return 0.0
    try:
        return float(val)
    except Exception as e:
        logger.warning(f"Unexpected value for float conversion: {val} — {e}")
        return 0.0
 
def validate_row(diff_val, pc_val, comm_val):
    try:
        if diff_val > 10_000_000:
            flag = "Fail"
        elif pc_val > 10 and diff_val >= 5_000_000:
            flag = "Fail"
        else:
            flag = "Pass"
        existence_status = "Pass" if flag == "Fail" and comm_val.strip() not in ["", "0", "0.0"] else "Fail"
        return flag, existence_status
    except Exception as e:
        logger.error(f"Validation error: {e}")
        return "Error", "Error"
 
# GN1811 logic
def process_gn1811():
    try:
        df = spark.read.format("delta").load(f"{base_path}/ods_gn_18_1_1").filter(col("txnID") == txn_id)
        if df.count() == 0:
            logger.warning("No GN1811 data found for txnID.")
            return None
    except Exception as e:
        logger.error(f"Error loading GN1811 sheet: {e}")
        return None
 
    rows = {}
    for label in ["difference", "% change", "commentary"]:
        try:
            row_df = df.filter(F.lower(F.col("fsli")) == label).drop("fsli", "row_no")
            if row_df.count() > 0:
                rows[label] = row_df.collect()[0].asDict()
        except Exception as e:
            logger.warning(f"Error extracting {label} row: {e}")
 
    try:
        id_val = df.filter(F.lower(F.col("fsli")) == "difference").select("id").first()["id"]
    except Exception as e:
        logger.error(f"Error extracting id from GN1811: {e}")
        id_val = "unknown"
 
    asset_cols = [c for c in df.columns if c not in ["fsli", "row_no", "txnID", "id"]]
    result = []
 
    for col_name in asset_cols:
        try:
            diff_val = abs(safe_float(rows.get("difference", {}).get(col_name, 0.0)))
            pc_val = abs(safe_float(rows.get("% change", {}).get(col_name, 0.0)))
            comm_val = str(rows.get("commentary", {}).get(col_name, "")) if rows.get("commentary", {}).get(col_name) else ""
            flag, existence_status = validate_row(diff_val, pc_val, comm_val)
            result.append(("GN1811", "0", "", col_name, diff_val, pc_val, comm_val, flag, existence_status, "", txn_id, id_val, id_val))
        except Exception as e:
            logger.error(f"Error processing column {col_name}: {e}")
 
    return spark.createDataFrame(result, [
        "sheetname", "mappingtableid", "fsli", "account", "difference", "pc_change", "commentary",
        "PassFailFlag", "Existence_Status", "Existence_Justification", "txnID", "odsid", "id"
    ])
 
# Mapping sheet logic
def process_mapping_sheets():
    try:
        mapping_df = spark.read.format("delta").load(f"{base_path}/core_rule4_mapping")
    except Exception as e:
        logger.error(f"Error loading mapping sheet: {e}")
        return None
 
    sheet_paths = {
        "GN-17.3": f"{base_path}/ods_gn_17_3",
        "SOCIE": f"{base_path}/ods_socie",
        "SoFP - Commentary": f"{base_path}/ods_sofp_commentary",
        "SoPL - Commentary": f"{base_path}/ods_sopl_commentary"
    }
 
    cached_sheets = {}
    final_dfs = []
 
    for row in mapping_df.collect():
        try:
            sheetname, account, diff_col, pc_col, comm_col, mappingtableid = row["sheetname"], row["account"], row["differenceheader"], row["changeheader"], row["commentaryheader"], row["id"]
            path = sheet_paths.get(sheetname)
            if not path:
                logger.warning(f"No path found for sheet: {sheetname}")
                continue
 
            if sheetname not in cached_sheets:
                try:
                    df = spark.read.format("delta").load(path).filter(col("txnID") == txn_id)
                    cached_sheets[sheetname] = df.cache() if df.count() > 0 else None
                except Exception as e:
                    logger.error(f"Error loading sheet {sheetname}: {e}")
                    cached_sheets[sheetname] = None
            df = cached_sheets.get(sheetname)
            if df is None:
                logger.warning(f"Sheet {sheetname}  has no data Corresponding to the given txnID .")
                continue
 
            missing_cols = [c for c in [diff_col, pc_col, comm_col] if c not in df.columns]
            if missing_cols:
                logger.warning(f"Sheet {sheetname} is missing required columns: {missing_cols}")
                continue
 
            if df.filter(col("txnID") == txn_id).count() == 0:
                logger.info(f"Sheet {sheetname} has no data for txnID: {txn_id}")
                continue
 
            df_casted = df.withColumn(diff_col, col(diff_col).cast("double")).withColumn(pc_col, col(pc_col).cast("double"))
            fail_1 = F.abs(col(diff_col)) > 10_000_000
            fail_2 = (F.abs(col(pc_col)) > 10) & (F.abs(col(diff_col)) >= 5_000_000)
 
            df_casted = df_casted.withColumn("PassFailFlag",
                F.when(col(diff_col).isNull() | isnan(col(diff_col)), "Not Applicable")
                 .when(fail_1 | fail_2, "Fail")
                 .otherwise("Pass")
            )
 
            formatted = df_casted.select(
                F.lit(sheetname).alias("sheetname"),
                F.lit(mappingtableid).alias("mappingtableid"),
                col("fsli"),
                F.lit(account).alias("account"),
                col(diff_col).alias("difference"),
                col(pc_col).alias("pc_change"),
                col(comm_col).alias("commentary"),
                col("PassFailFlag"),
                F.when(
                    (col("PassFailFlag") == "Fail") &
                    (col(comm_col).isNotNull()) &
                    (~isnan(col(comm_col))) &
                    (~F.trim(col(comm_col)).isin("", "0", "0.0")),
                    "Pass"
                ).otherwise("Fail").alias("Existence_Status"),
                F.lit("").alias("Existence_Justification"),
                F.lit(txn_id).alias("txnID"),
                col("id").alias("odsid"),
                col("id").alias("id")
            )
 
            final_dfs.append(formatted)
        except Exception as e:
            logger.error(f"Error processing mapping row: {e}")
 
    return reduce(DataFrame.unionByName, final_dfs) if final_dfs else None
 
# Combine and write
try:
    gn1811_df = process_gn1811()
    mapping_df = process_mapping_sheets()
    combined_df = reduce(DataFrame.unionByName, [df for df in [gn1811_df, mapping_df] if df is not None]) if gn1811_df or mapping_df else None
 
    if combined_df:
        window = Window.orderBy(F.monotonically_increasing_id())
        final_df = combined_df.withColumn("row_num", F.row_number().over(window)) \
                              .withColumn("id", F.concat(F.lit("AI1_"), F.format_string("%06d", col("row_num")))) \
                              .drop("row_num")
 
        cols = ["id"] + [c for c in final_df.columns if c != "id"]
        final_df.select(*cols).write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(final_output_path)
        logger.info(" Final output written successfully.")
    else:
        logger.warning(" No valid data found to write.")
except Exception as e:
    logger.critical(f"Unhandled error during final write: {e}")
 
























F.when(
    (F.col("PassFailFlag") == "Fail") &
    (F.col(comm_col).isNotNull()) &
    (~isnan(F.col(comm_col))) &
    (~F.trim(F.col(comm_col)).isin("", "0", "0.0")),
    "Pass"
).otherwise("Fail").alias("Existence_Status"),

F.lit("").alias("Existence_Justification")


�� ADDH Intercompany dividend payable

joined_df = joined_df.withColumn(
    "fsli",
    F.regexp_replace(F.regexp_replace(F.regexp_replace("fsli", r"[^\x00-\x7F]", ""), r"\\s+", " "), r"^\\s+|\\s+$", "")
)


from openpyxl import load_workbook
from openpyxl.workbook.calc_props import CalcProperties

def enable_auto_calc(file_in, file_out):
    wb = load_workbook(file_in)

    # Check if _calcPr exists, if not, create one
    if not hasattr(wb, '_calcPr') or wb._calcPr is None:
        wb._calcPr = CalcProperties(calcMode='auto', fullCalcOnLoad=True)
    else:
        wb._calcPr.calcMode = 'auto'
        wb._calcPr.fullCalcOnLoad = True

    wb.save(file_out)
    print(f"Saved workbook with automatic calculation: {file_out}")

# Example usage
enable_auto_calc('input.xlsx', 'output_auto_calc.xlsx')




import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Find workbookPr element and update calcMode attribute
    # Excel namespace
    ns = {'ns': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}
    workbookPr = root.find('ns:workbookPr', ns)
    
    if workbookPr is None:
        # If not found, create it
        workbookPr = etree.Element('{http://schemas.openxmlformats.org/spreadsheetml/2006/main}workbookPr')
        root.insert(0, workbookPr)
    
    # Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 5. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 6. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                # archive name should be relative to temp_dir
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 7. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage example
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # 1. Extract the XLSX contents to a temp directory
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    
    # 2. Path to the workbook.xml inside extracted folder
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')
    
    # 3. Parse workbook.xml with lxml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()
    
    # 4. Extract the namespace from root tag, e.g. '{namespace}workbook'
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    ns = {'ns': ns_uri}
    
    # 5. Find workbookPr element with the correct namespace
    workbookPr = root.find('ns:workbookPr', namespaces=ns)
    
    if workbookPr is None:
        # Create workbookPr if it doesn't exist
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)
    
    # 6. Set calcMode attribute to auto
    workbookPr.set('calcMode', 'auto')
    
    # 7. Write back the modified XML
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')
    
    # 8. Re-zip the folder contents into a new XLSX file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)
    
    # 9. Cleanup temp directory
    shutil.rmtree(temp_dir)

# Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')





import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'
    
    # Clean temp dir if it exists
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract the .xlsx contents
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Path to workbook.xml
    workbook_xml_path = os.path.join(temp_dir, 'xl', 'workbook.xml')

    # Step 3: Parse workbook.xml
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml_path, parser)
    root = tree.getroot()

    # Step 4: Extract the namespace dynamically
    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 5: Find or create <workbookPr>
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 6: Set or update calcMode to "auto"
    workbookPr.set('calcMode', 'auto')

    # Optional: Remove manual-related flags that force manual behavior
    for attr in ['fullCalcOnLoad', 'forceFullCalc']:
        if attr in workbookPr.attrib:
            del workbookPr.attrib[attr]

    # Step 7: Save the modified workbook.xml
    tree.write(workbook_xml_path, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 8: Repackage everything into a new .xlsx file
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for foldername, subfolders, filenames in os.walk(temp_dir):
            for filename in filenames:
                file_path = os.path.join(foldername, filename)
                archive_name = os.path.relpath(file_path, temp_dir)
                zip_out.write(file_path, archive_name)

    # Step 9: Cleanup
    shutil.rmtree(temp_dir)

# ✅ Usage
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')






import zipfile
import shutil
import os
from lxml import etree

def set_calc_mode_auto(xlsx_path, output_path):
    temp_dir = 'temp_xlsx'

    # Clean up and create temp dir
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)

    # Step 1: Extract
    with zipfile.ZipFile(xlsx_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    # Step 2: Modify workbook.xml
    workbook_xml = os.path.join(temp_dir, 'xl', 'workbook.xml')
    parser = etree.XMLParser(remove_blank_text=True)
    tree = etree.parse(workbook_xml, parser)
    root = tree.getroot()

    ns_uri = root.tag[root.tag.find("{")+1 : root.tag.find("}")]
    nsmap = {'ns': ns_uri}

    # Step 3: Find/create workbookPr
    workbookPr = root.find('ns:workbookPr', namespaces=nsmap)
    if workbookPr is None:
        workbookPr = etree.Element(f'{{{ns_uri}}}workbookPr')
        root.insert(0, workbookPr)

    # Step 4: Set calcMode and cleanup other attributes
    workbookPr.attrib.clear()  # clear all attributes
    workbookPr.set('calcMode', 'auto')

    # Optional: Add Excel default calcId (can help with Excel Online)
    workbookPr.set('calcId', '122211')  # example value from new Excel files

    # Step 5: Write back
    tree.write(workbook_xml, pretty_print=True, xml_declaration=True, encoding='UTF-8')

    # Step 6: Re-zip to output_path
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_out:
        for root_dir, dirs, files in os.walk(temp_dir):
            for file in files:
                abs_path = os.path.join(root_dir, file)
                rel_path = os.path.relpath(abs_path, temp_dir)
                zip_out.write(abs_path, rel_path)

    # Step 7: Cleanup
    shutil.rmtree(temp_dir)

# Example usage (paths must point to accessible Lakehouse or OneLake locations)
set_calc_mode_auto('input_workbook.xlsx', 'output_workbook_auto_calc.xlsx')
