
#final gn28

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Define input and output paths
input_table_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables/ods_gn_28"
output_table_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables/rule4_gn_28"

# Load source table from abfss path (assuming Delta format)
df = spark.read.format("delta").load(input_table_path)

# Cast 'difference' and 'pc_change' to numeric
df = df.withColumn("difference", F.col("difference").cast("double"))
df = df.withColumn("pc_change", F.col("pc_change").cast("double"))

# Exclude specific fsli values
excluded_fsli = [
    "at period end",
    "current",
    "",
    "as per sofp",
    "non-current &amp; current total",
    "validation",
    "non-current"
]

df = df.filter(~F.lower(F.trim(F.col("fsli"))).isin(excluded_fsli))

# Reverse order and mark GN 28 rows
df = df.withColumn("reverse_row_no", F.row_number().over(Window.orderBy(F.col("row_no").desc())))
df = df.withColumn("is_gn28", F.col("fsli").startswith("+GN 28"))

# Create block ID using cumulative sum
df = df.withColumn("block_id", F.sum(F.when(F.col("is_gn28"), 1).otherwise(0)).over(
    Window.orderBy("reverse_row_no").rowsBetween(Window.unboundedPreceding, 0)
))

# Define fail conditions
fail_condition_1 = F.col("difference") > 10_000_000
fail_condition_2 = (F.col("pc_change") > 10) & (F.col("difference") >= 5_000_000)
fail_condition = fail_condition_1 | fail_condition_2

# Add pass/fail flag (only show "Fail", keep blank for "Pass")
df = df.withColumn("pass_fail_flag", F.when(fail_condition, "Fail").otherwise("Pass"))

# Add remark column with actual values for failed rows, null for passed
df = df.withColumn(
    "remark",
    F.when(
        fail_condition_1,
        F.concat(
            F.lit("Amount exceeds threshold. difference="),
            F.concat(F.round(F.col("difference") / 1_000_000, 2), F.lit("M"))
        )
    ).when(
        fail_condition_2,
        F.concat_ws(
            ", ",
            F.lit("% Change and Amount exceed thresholds."),
            F.concat(F.lit("pc_change="), F.round(F.col("pc_change"), 2)),
            F.concat(F.lit("difference="), F.concat(F.round(F.col("difference") / 1_000_000, 2), F.lit("M")))
        )
    ).otherwise(F.lit(None))
)
commentary_df = df.filter(
    F.trim(F.col("current_period_commentary")).isNotNull() &
    (F.trim(F.col("current_period_commentary")) != "") &
    (F.trim(F.col("current_period_commentary")) != "0")
).groupBy("block_id").agg(
    F.concat_ws(" || ", F.collect_list("current_period_commentary")).alias("current_period_commentary")
)


# Drop commentary from main df to avoid ambiguity during join
df = df.drop("current_period_commentary")



result_df = df.filter(F.col("fsli").startswith("+GN 28")) \
    .join(commentary_df, on="block_id", how="left") \
    .select(
        "id",
        "txnID",
        "fsli",
        "difference",
        "pc_change",
        "current_period_commentary",
        "row_no",
        "pass_fail_flag",
        "remark"
    )
# Save result to Lakehouse (Delta format)
result_df.write.format("delta").mode("overwrite").save(output_table_path)


# Drop commentary from main df to avoid ambiguity during join
df = df.drop("current_period_commentary")



result_df = df.filter(F.col("fsli").startswith("+GN 28")) \
    .join(commentary_df, on="block_id", how="left") \
    .select(
        "id",
        "txnID",
        "fsli",
        "difference",
        "pc_change",
        "current_period_commentary",
        "row_no",
        "pass_fail_flag",
        "remark"
    )
# Save result to Lakehouse (Delta format)
result_df.write.format("delta").mode("overwrite").save(output_table_path)




#final input to csv

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.window import Window

spark = SparkSession.builder \
    .appName("GN28 AI Input Filter") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.4.0") \
    .getOrCreate()

table1_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables/ods_gn_28"
table2_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Tables/rule4_gn_28"
ai_output_csv_path = "abfss://b20f828f-5169-4a6e-869e-ea6bb5e75572@onelake.dfs.fabric.microsoft.com/2d2bf4a9-53b4-40ed-a361-440c70e15ae7/Files/testing"

df1 = spark.read.format("delta").load(table1_path)
df2 = spark.read.format("delta").load(table2_path)

joined_df = df1.join(
    df2.select("id", "txnID", "pass_fail_flag", "remark"),
    on=["id", "txnID"],
    how="left"
)

joined_df = joined_df.withColumn(
    "is_parent", F.when(F.col("fsli").startswith("+GN 28"), 1).otherwise(0)
)

window_spec = Window.orderBy(F.desc("row_no"))
joined_df = joined_df.withColumn(
    "reverse_group_id", F.sum("is_parent").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))
)

failing_groups = joined_df.filter(F.col("pass_fail_flag") == "Fail") \
                          .select("reverse_group_id").distinct()

ai_input_df = joined_df.join(failing_groups, on="reverse_group_id", how="inner")

excluded_fsli = [
    "at period end",
    "current",
    "",
    "as per sofp",
    "non-current & current total",
    "validation",
    "non-current"
]

ai_input_df = ai_input_df.withColumn("fsli_clean", F.lower(F.trim(F.col("fsli"))))
ai_input_df = ai_input_df.filter(~F.col("fsli_clean").isin(excluded_fsli)).drop("fsli_clean")

ai_input_df = ai_input_df.orderBy("reverse_group_id", "is_parent", "row_no")

ai_input_df = ai_input_df.selectai_input_df = ai_input_df.select(
    "fsli", "difference", "pc_change",
    "current_period", "prior_period",  
    "current_period_commentary"
)

ai_input_df.coalesce(1).write.mode("overwrite").option("header", True).csv(ai_output_csv_path)

print(f" AI input CSV saved at: {ai_output_csv_path}")




