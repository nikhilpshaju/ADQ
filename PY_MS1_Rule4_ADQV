import pandas as pd
from pyspark.sql import SparkSession
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, concat, trim, regexp_extract, when
from pyspark.sql.types import StringType, DoubleType



---------------------------------------------------------------------------------


#final gn28
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.functions import col
txn_id = 'MS1_000134'

spark = SparkSession.builder \
        .appName("ADQFabricLHDev01") \
        .config("spark.jars.packages", "io.delta:delta-core_2.12:<delta-version>") \
        .getOrCreate()



# Define input and output paths
input_table_path = "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Tables/ods_gn_28"
output_table_path = "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Tables/rule4_gn_28"

# Load source table from abfss path (assuming Delta format)

sdf = spark.read.format("delta").load(input_table_path).filter(
    col("txnID") == txn_id
).createOrReplaceTempView("filtered_ods")

df = spark.sql("SELECT * FROM filtered_ods")
df.show()


# Cast 'difference' and 'pc_change' to numeric
df = df.withColumn("difference", F.col("difference").cast("double"))
df = df.withColumn("pc_change", F.col("pc_change").cast("double"))

# Exclude specific fsli values
excluded_fsli = [
    "at period end",
    "current",
    "",
    "as per sofp",
    "non-current &amp; current total",
    "validation",
    "non-current"
]

df = df.filter(~F.lower(F.trim(F.col("fsli"))).isin(excluded_fsli))

# Reverse order and mark GN 28 rows
df = df.withColumn("reverse_row_no", F.row_number().over(Window.orderBy(F.col("row_no").desc())))
df = df.withColumn("is_gn28", F.col("fsli").startswith("+GN 28"))

# Create block ID using cumulative sum
df = df.withColumn("block_id", F.sum(F.when(F.col("is_gn28"), 1).otherwise(0)).over(
    Window.orderBy("reverse_row_no").rowsBetween(Window.unboundedPreceding, 0)
))

# Define fail conditions
fail_condition_1 = F.col("difference") > 10_000_000
fail_condition_2 = (F.col("pc_change") > 10) & (F.col("difference") >= 5_000_000)
fail_condition = fail_condition_1 | fail_condition_2

# Add pass/fail flag (only show "Fail", keep blank for "Pass")
df = df.withColumn("pass_fail_flag", F.when(fail_condition, "Fail").otherwise("Pass"))

# Add remark column with actual values for failed rows, null for passed
df = df.withColumn(
    "remark",
    F.when(
        fail_condition_1,
        F.concat(
            F.lit("Amount exceeds threshold. difference="),
            F.concat(F.round(F.col("difference") / 1_000_000, 2), F.lit("M"))
        )
    ).when(
        fail_condition_2,
        F.concat_ws(
            ", ",
            F.lit("% Change and Amount exceed thresholds."),
            F.concat(F.lit("pc_change="), F.round(F.col("pc_change"), 2)),
            F.concat(F.lit("difference="), F.concat(F.round(F.col("difference") / 1_000_000, 2), F.lit("M")))
        )
    ).otherwise(F.lit(None))
)
commentary_df = df.filter(
    F.trim(F.col("current_period_commentary")).isNotNull() &
    (F.trim(F.col("current_period_commentary")) != "") &
    (F.trim(F.col("current_period_commentary")) != "0")
).groupBy("block_id").agg(
    F.concat_ws(" || ", F.collect_list("current_period_commentary")).alias("current_period_commentary")
)


# Drop commentary from main df to avoid ambiguity during join
df = df.drop("current_period_commentary")



result_df = df.filter(F.col("fsli").startswith("+GN 28")) \
    .join(commentary_df, on="block_id", how="left") \
    .select(
        "id",
        "txnID",
        "fsli",
        "difference",
        "pc_change",
        "current_period_commentary",
        "row_no",
        "pass_fail_flag",
        "remark"
    )
# Save result to Lakehouse (Delta format)
result_df.write.format("delta").mode("overwrite").save(output_table_path)






-----------------------------------------------------------------------------------------------------



#final input to csv

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.window import Window

spark = SparkSession.builder \
    .appName("GN28 AI Input Filter") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.4.0") \
    .getOrCreate()

table1_path = "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Tables/ods_gn_28"
table2_path = "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Tables/rule4_gn_28"
ai_output_csv_path = "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Files/Rule4testing"

df1 = spark.read.format("delta").load(table1_path)
df2 = spark.read.format("delta").load(table2_path)

joined_df = df1.join(
    df2.select("id", "txnID", "pass_fail_flag", "remark"),
    on=((df1["id"] == df2["id"]) & (df1["txnID"] == df2["txnID"])),
    how="left" 
).where(df1["txnID"]==txn_id)

print(joined_df.count())
#joined_df.show()

joined_df = joined_df.withColumn(
    "is_parent", F.when(F.col("fsli").startswith("+GN 28"), 1).otherwise(0)
)

window_spec = Window.orderBy(F.desc("row_no"))
joined_df = joined_df.withColumn(
    "reverse_group_id", F.sum("is_parent").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))
)

failing_groups = joined_df.filter(F.col("pass_fail_flag") == "Fail") \
                          .select("reverse_group_id").distinct()

ai_input_df = joined_df.join(failing_groups, on="reverse_group_id", how="inner")

excluded_fsli = [
    "at period end",
    "current",
    "",
    "as per sofp",
    "non-current & current total",
    "validation",
    "non-current"
]

ai_input_df = ai_input_df.withColumn("fsli_clean", F.lower(F.trim(F.col("fsli"))))
ai_input_df = ai_input_df.filter(~F.col("fsli_clean").isin(excluded_fsli)).drop("fsli_clean")

ai_input_df = ai_input_df.orderBy("reverse_group_id", "is_parent", "row_no")

ai_input_df = ai_input_df.select(
    "fsli", "difference", "pc_change",
    "current_period", "prior_period",  
    "current_period_commentary"
)

ai_input_df.coalesce(1).write.mode("overwrite").option("header", True).csv(ai_output_csv_path)

print(f" AI input CSV saved at: {ai_output_csv_path}")








----------------------------------------------------------------------------------

# This should be run after AI invokation
# final AI to table output 
# 1. Setup Spark session and define file paths

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.getOrCreate()

json_path = (
    "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Files/Rule4testingOut/all_results.json"
)

existing_table_path = (
    "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Tables/rule4_gn_28"
)

new_table_path = (
    "abfss://b74ee0ac-c42f-4c9d-be97-62f6f20d87f1@onelake.dfs.fabric.microsoft.com/1aeac658-239d-4d53-af2b-14eb64c18a19/Tables/AIIrule_gn28"
)


# 2. Read the JSON validation result
json_df = spark.read.option("multiline", True).json(json_path)

# 3. Extract and rename columns with AI_ prefix to avoid duplication
flattened_df = (
    json_df.select(
        col("Sub-FSLI").alias("fsli"),

        col("Existence.status").alias("Existence_Status"),
        col("Existence.justification").alias("Existence_Justification"),

        col("Completeness.status").alias("Completeness_Status"),
        col("Completeness.justification").alias("Completeness_Justification"),

        col("Accuracy.status").alias("Accuracy_Status"),
        col("Accuracy.reasoning_check").alias("Accuracy_Justification"),

        col("Overall_Justification").alias("Overall_Justification"),
        col("Remarks").alias("final_remarks")
    )
)

# 4. Read existing Fabric table
existing_df = spark.read.format("delta").load(existing_table_path)

# 5. Join JSON data with existing table on fsli
merged_df = (
    existing_df.join(
        flattened_df,
        existing_df["fsli"] == flattened_df["fsli"],
        how="left"
    )
    .drop(flattened_df["fsli"])
)

# 6. Save merged data as a NEW Delta table
merged_df.write.format("delta").mode("overwrite").save(new_table_path)
